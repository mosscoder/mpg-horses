{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Fundamentals\n",
    "\n",
    "This notebook covers the fundamentals of working with PyTorch for deep learning, including:\n",
    "- Loading data from Hugging Face\n",
    "- Applying proper transformations and normalization\n",
    "- Using pre-trained models (ResNet50)\n",
    "- Setting up training with appropriate optimization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for or install all necessary packages with conda from environment.yml\n",
    "# %conda env update -f ../environment.yml\n",
    "\n",
    "# For colab\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset, Features, Value\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "# Import F1 score calculation\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face with token if necessary\n",
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"mpg-ranch/horse-detection\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation split from the training data\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"image\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preprocessing Requirements\n",
    "\n",
    "First, let's examine what preprocessing is expected by the pretrained ResNet50 model. We'll check the metadata from the model weights to understand the expected input size, normalization values, and default transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessing transforms directly from the weights\n",
    "weights = ResNet50_Weights.IMAGENET1K_V1\n",
    "default_transforms = weights.transforms()\n",
    "\n",
    "# Print available metadata keys\n",
    "print(\"Available metadata keys:\")\n",
    "if hasattr(weights, 'meta'):\n",
    "    print(list(weights.meta.keys()))\n",
    "else:\n",
    "    print(\"No 'meta' attribute found\")\n",
    "\n",
    "# Print the preprocessing information safely\n",
    "print(\"\\nModel expects the following preprocessing:\")\n",
    "try:\n",
    "    # Try different possible key names for input size\n",
    "    if hasattr(weights, 'meta'):\n",
    "        if 'input_size' in weights.meta:\n",
    "            print(f\"- Input size: {weights.meta['input_size']}\")\n",
    "        elif 'imageSize' in weights.meta:\n",
    "            print(f\"- Input size: {weights.meta['imageSize']}\")\n",
    "        else:\n",
    "            print(\"- Input size: Not found in metadata\")\n",
    "            \n",
    "        # Try different possible key names for mean/std\n",
    "        if 'mean' in weights.meta:\n",
    "            print(f\"- Mean: {weights.meta['mean']}\")\n",
    "        else:\n",
    "            print(\"- Mean: [0.485, 0.456, 0.406] (ImageNet standard)\")\n",
    "            \n",
    "        if 'std' in weights.meta:\n",
    "            print(f\"- Std: {weights.meta['std']}\")\n",
    "        else:\n",
    "            print(\"- Std: [0.229, 0.224, 0.225] (ImageNet standard)\")\n",
    "    else:\n",
    "        print(\"Metadata not available, using standard ImageNet values:\")\n",
    "        print(\"- Input size: [3, 224, 224]\")\n",
    "        print(\"- Mean: [0.485, 0.456, 0.406]\")\n",
    "        print(\"- Std: [0.229, 0.224, 0.225]\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing metadata: {e}\")\n",
    "    print(\"Using standard ImageNet values:\")\n",
    "    print(\"- Input size: [3, 224, 224]\")\n",
    "    print(\"- Mean: [0.485, 0.456, 0.406]\")\n",
    "    print(\"- Std: [0.229, 0.224, 0.225]\")\n",
    "\n",
    "# Print information about the default transforms\n",
    "print(\"\\nDefault transforms from weights:\")\n",
    "print(default_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transforms for Our Dataset\n",
    "\n",
    "Based on the model requirements, we'll now define our custom transforms for both training and validation datasets:\n",
    "\n",
    "1. **Training transforms**: Include data augmentation (random crops, flips, rotation) to improve model generalization, along with the required normalization\n",
    "2. **Validation transforms**: Only include deterministic center cropping and normalization (no augmentation) to evaluate the model on consistent inputs\n",
    "\n",
    "Note the specific cropping approach:\n",
    "- Training: 115px center crop followed by a 77px random crop to simulate our camera setup and add variation\n",
    "- Validation: 77px deterministic center crop (1m equivalent in our camera setup) for consistent evaluation\n",
    "\n",
    "Both transforms include the same normalization parameters to match the pretrained model's expectations, but validation excludes augmentation to ensure consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training data (augmentations + the model's preprocessing)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(115),  # First, center crop to 1.5m (115 pixels)\n",
    "    transforms.RandomCrop(77),   # Then, random crop to 1m (77 pixels)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Validation transformations (no augmentation, only resize and normalize)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(77),   # Deterministic crop to 1m (77 pixels)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Print your custom transforms\n",
    "print(\"Custom transforms defined for training and validation:\")\n",
    "print(\"\\nTraining transforms:\")\n",
    "print(train_transforms)\n",
    "print(\"\\nValidation transforms:\")\n",
    "print(val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Transformations to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply transformations to the training dataset\n",
    "def transform_train_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Function to apply transformations to the validation dataset\n",
    "def transform_val_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        val_transforms(image.convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Apply transformations to training set\n",
    "transformed_train_dataset = train_dataset.map(\n",
    "    transform_train_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\"]  # Remove original images after transformation\n",
    ")\n",
    "\n",
    "# Apply transformations to test set\n",
    "transformed_val_dataset = val_dataset.map(\n",
    "    transform_val_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\"]\n",
    ")\n",
    "\n",
    "# Set the format for PyTorch\n",
    "transformed_train_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])\n",
    "transformed_val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    transformed_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    transformed_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Batch\n",
    "\n",
    "Let's visualize some images from our dataloader to verify transformations are applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize images for visualization\n",
    "def denormalize(tensor):\n",
    "    # Make sure tensor is the right shape and type\n",
    "    if tensor.ndim != 3:  # If not a single image with 3 dimensions (C,H,W)\n",
    "        if tensor.ndim == 4:  # If it's a batch of images (B,C,H,W)\n",
    "            tensor = tensor[0]  # Take the first image\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected tensor shape: {tensor.shape}\")\n",
    "    \n",
    "    # Make sure we're working with the image tensor, not other data\n",
    "    if tensor.shape[0] == 3:  # If first dimension is 3, it's likely the channel dimension\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        return tensor * std + mean\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 3 channels, got {tensor.shape[0]}\")\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# Check the structure of the batch\n",
    "print(f\"Batch type: {type(batch)}\")\n",
    "if isinstance(batch, dict):\n",
    "    print(f\"Batch keys: {batch.keys()}\")\n",
    "    images = batch['pixel_values']  # Adjust based on your actual key\n",
    "    Presences = batch['Presence']  # Adjust based on your actual key\n",
    "else:\n",
    "    # If it's a tuple or list, unpack accordingly\n",
    "    images, Presences = batch\n",
    "\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Presences shape: {Presences.shape}\")\n",
    "\n",
    "# Visualize a few images from the batch\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(images):\n",
    "        # Denormalize the image\n",
    "        img = denormalize(images[i])\n",
    "        img = img.permute(1, 2, 0).numpy()  # Change from CxHxW to HxWxC\n",
    "        img = np.clip(img, 0, 1)  # Clip values to valid range\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Presence: {Presences[i].item()}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Unfreeze all layers from the start\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Modify the final fully connected layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  # Output a single value\n",
    "\n",
    "# Print model architecture summary\n",
    "print(f\"Model: ResNet50\")\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "We'll use default settings for the optimizer and only tune the learning rate and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function for binary classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Use a smaller learning rate for fine-tuning all layers\n",
    "learning_rate = 0.0001  # Reduced from 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Number of epochs is another parameter we'll tune\n",
    "num_epochs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rates to test\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# Function to train model with a specific learning rate\n",
    "def train_with_learning_rate(lr, num_epochs=3):\n",
    "    # Initialize a fresh model (using the same architecture)\n",
    "    test_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Unfreeze all layers\n",
    "    for param in test_model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Modify the final fully connected layer for binary classification\n",
    "    test_model.fc = nn.Linear(test_model.fc.in_features, 1)\n",
    "    test_model = test_model.to(device)\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Use the specified learning rate\n",
    "    optimizer = optim.Adam(test_model.parameters(), lr=lr)\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    # Run for fewer epochs when testing learning rates\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        test_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            # Extract data\n",
    "            if isinstance(batch, dict):\n",
    "                images = batch['pixel_values']\n",
    "                Presences = batch['Presence']\n",
    "            else:\n",
    "                images, Presences = batch\n",
    "            \n",
    "            # Move to device and prepare\n",
    "            images, Presences = images.to(device), Presences.to(device)\n",
    "            Presences = Presences.float()\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = test_model(images)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, Presences)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        test_model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                # Extract data\n",
    "                if isinstance(batch, dict):\n",
    "                    images = batch['pixel_values']\n",
    "                    Presences = batch['Presence']\n",
    "                else:\n",
    "                    images, Presences = batch\n",
    "                \n",
    "                # Move to device and prepare\n",
    "                images, Presences = images.to(device), Presences.to(device)\n",
    "                Presences = Presences.float()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = test_model(images)\n",
    "                outputs = outputs.squeeze(1)\n",
    "                loss = criterion(outputs, Presences)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                val_running_loss += loss.item()\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                total += Presences.size(0)\n",
    "                correct += predicted.eq(Presences).sum().item()\n",
    "                \n",
    "                # Store predictions for F1 score\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(Presences.cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss = val_running_loss / len(val_dataloader)\n",
    "        val_acc = 100. * correct / total\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "        \n",
    "        # Store metrics\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'LR: {lr:.6f}, Epoch: {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'val_f1s': val_f1s,\n",
    "        'final_val_acc': val_accs[-1],\n",
    "        'final_val_f1': val_f1s[-1]\n",
    "    }\n",
    "\n",
    "# Set device (this should match the one used in the main training loop)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dictionary to store results\n",
    "lr_results = {}\n",
    "\n",
    "# Test each learning rate\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "    lr_results[lr] = train_with_learning_rate(lr)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(2, 2, 1)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(range(1, len(lr_results[lr]['train_losses'])+1), \n",
    "             lr_results[lr]['train_losses'], \n",
    "             label=f'LR: {lr}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss by Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(2, 2, 2)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(range(1, len(lr_results[lr]['val_losses'])+1), \n",
    "             lr_results[lr]['val_losses'], \n",
    "             label=f'LR: {lr}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss by Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.subplot(2, 2, 3)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(range(1, len(lr_results[lr]['val_accs'])+1), \n",
    "             lr_results[lr]['val_accs'], \n",
    "             label=f'LR: {lr}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Validation Accuracy by Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation F1 score\n",
    "plt.subplot(2, 2, 4)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(range(1, len(lr_results[lr]['val_f1s'])+1), \n",
    "             lr_results[lr]['val_f1s'], \n",
    "             label=f'LR: {lr}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation F1 Score')\n",
    "plt.title('Validation F1 Score by Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Learning Rate Performance:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Learning Rate':<15} {'Final Val Acc':<15} {'Final Val F1':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for lr in learning_rates:\n",
    "    print(f\"{lr:<15.6f} {lr_results[lr]['final_val_acc']:<15.2f} {lr_results[lr]['final_val_f1']:<15.4f}\")\n",
    "\n",
    "# Identify best learning rate based on validation accuracy\n",
    "best_lr = max(learning_rates, key=lambda lr: lr_results[lr]['final_val_acc'])\n",
    "print(f\"\\nBest learning rate based on validation accuracy: {best_lr}\")\n",
    "\n",
    "# Recommended learning rate to use in the main training loop\n",
    "print(f\"\\nRecommendation: Use learning_rate = {best_lr} in the main training configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "train_f1s = []  # New list for training F1 scores\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "val_f1s = []    # New list for validation F1 scores\n",
    "\n",
    "# Check the structure of a batch to understand the data format\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"Batch type: {type(sample_batch)}\")\n",
    "if isinstance(sample_batch, dict):\n",
    "    print(f\"Batch keys: {sample_batch.keys()}\")\n",
    "    # Adjust these based on your actual keys\n",
    "    image_key = 'pixel_values' if 'pixel_values' in sample_batch else 'img'\n",
    "    Presence_key = 'Presence' if 'Presence' in sample_batch else 'Presences'\n",
    "    print(f\"Using keys - Images: '{image_key}', Presences: '{Presence_key}'\")\n",
    "else:\n",
    "    print(f\"Batch is a {type(sample_batch)} with {len(sample_batch)} elements\")\n",
    "    for i, item in enumerate(sample_batch):\n",
    "        print(f\"  Item {i} type: {type(item)}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_train_preds = []  # To store all training predictions\n",
    "    all_train_labels = [] # To store all training labels\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Extract images and Presences based on the batch structure\n",
    "        if isinstance(batch, dict):\n",
    "            # Dictionary format (common with Hugging Face datasets)\n",
    "            images = batch['pixel_values'] if 'pixel_values' in batch else batch['img']\n",
    "            Presences = batch['Presence'] if 'Presence' in batch else batch['Presences']\n",
    "        else:\n",
    "            # Tuple/list format (common with PyTorch datasets)\n",
    "            images, Presences = batch\n",
    "\n",
    "        # Move data to device\n",
    "        images, Presences = images.to(device), Presences.to(device)\n",
    "\n",
    "        # Convert Presences to float for BCEWithLogitsLoss\n",
    "        Presences = Presences.float()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.squeeze(1)  # Change from [batch_size, 1] to [batch_size]\n",
    "        loss = criterion(outputs, Presences)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # For binary classification with BCEWithLogitsLoss\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total += Presences.size(0)\n",
    "        correct += predicted.eq(Presences).sum().item()\n",
    "\n",
    "        # Collect predictions and labels for F1 calculation\n",
    "        all_train_preds.extend(predicted.cpu().numpy())\n",
    "        all_train_labels.extend(Presences.cpu().numpy())\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_f1 = f1_score(all_train_labels, all_train_preds, average='binary')\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_f1s.append(train_f1)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_val_preds = []    # To store all validation predictions\n",
    "    all_val_labels = []   # To store all validation labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # Extract images and Presences based on the batch structure\n",
    "            if isinstance(batch, dict):\n",
    "                # Dictionary format (common with Hugging Face datasets)\n",
    "                images = batch['pixel_values'] if 'pixel_values' in batch else batch['img']\n",
    "                Presences = batch['Presence'] if 'Presence' in batch else batch['Presences']\n",
    "            else:\n",
    "                # Tuple/list format (common with PyTorch datasets)\n",
    "                images, Presences = batch\n",
    "\n",
    "            # Move data to device\n",
    "            images, Presences = images.to(device), Presences.to(device)\n",
    "\n",
    "            # Convert Presences to float for BCEWithLogitsLoss\n",
    "            Presences = Presences.float()\n",
    "\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.squeeze(1)  # Change from [batch_size, 1] to [batch_size]\n",
    "            loss = criterion(outputs, Presences)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            # For binary classification with BCEWithLogitsLoss\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += Presences.size(0)\n",
    "            correct += predicted.eq(Presences).sum().item()\n",
    "\n",
    "            # Collect predictions and labels for F1 calculation\n",
    "            all_val_preds.extend(predicted.cpu().numpy())\n",
    "            all_val_labels.extend(Presences.cpu().numpy())\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    val_loss = val_running_loss / len(val_dataloader)\n",
    "    val_acc = 100. * correct / total\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='binary')\n",
    "\n",
    "    # Store metrics\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    # Print epoch results including F1 scores\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Train F1: {train_f1:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation metrics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "ax1.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(range(1, num_epochs+1), train_accs, label='Train Accuracy')\n",
    "ax2.plot(range(1, num_epochs+1), val_accs, label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points to Remember\n",
    "\n",
    "1. **Data Normalization is Critical**\n",
    "   - Always normalize your input data using mean and standard deviation\n",
    "   - For transfer learning with pre-trained models, use the same normalization values that were used during pre-training (e.g., ImageNet stats)\n",
    "\n",
    "2. **Data Transformations**\n",
    "   - Apply appropriate augmentations for training data (flips, rotations, crops)\n",
    "   - Use only resizing and normalization for validation/test data\n",
    "   - Transformations help prevent overfitting and improve model generalization\n",
    "\n",
    "3. **Model Architecture**\n",
    "   - Start with a pre-trained model like ResNet50\n",
    "   - Modify only the final layer (head) to match your specific task\n",
    "   - Freeze pre-trained layers initially to leverage transfer learning\n",
    "\n",
    "4. **Optimization Settings**\n",
    "   - Start with default optimizer settings\n",
    "   - Focus on tuning learning rate and number of epochs first\n",
    "   - Monitor validation metrics to prevent overfitting\n",
    "\n",
    "5. **Progressive Unfreezing**\n",
    "   - After initial training, you can unfreeze more layers gradually\n",
    "   - Use a smaller learning rate when fine-tuning pre-trained layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Hyperparameter Tuning**\n",
    "   - Try different learning rates\n",
    "   - Experiment with different batch sizes\n",
    "   - Test different optimizers (SGD with momentum, AdamW)\n",
    "\n",
    "2. **Model Improvements**\n",
    "   - Unfreeze more layers for fine-tuning\n",
    "   - Try different pre-trained architectures (EfficientNet, ViT)\n",
    "   - Implement learning rate scheduling\n",
    "\n",
    "3. **Advanced Techniques**\n",
    "   - Implement data augmentation strategies like mixup or cutmix\n",
    "   - Try different loss functions\n",
    "   - Implement ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthomosaic Testing Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading dataset...\n",
      "Found 10 orthomosaics\n",
      "\n",
      "Training model for orthomosaic: 240424_upperpartridge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6c18de46754441ac6a3c4b73694ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba16deebb47a4f068d3f01194718440d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 183\u001b[39m\n\u001b[32m    180\u001b[39m all_val_labels = []\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpixel_values\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpresences\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPresence\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/mpg-horses/lib/python3.13/site-packages/torch/utils/data/dataloader.py:484\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    482\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/mpg-horses/lib/python3.13/site-packages/torch/utils/data/dataloader.py:415\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    414\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/mpg-horses/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1138\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1131\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1132\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1140\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/mpg-horses/lib/python3.13/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/mpg-horses/lib/python3.13/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/mpg-horses/lib/python3.13/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/mpg-horses/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/mpg-horses/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/mpg-horses/lib/python3.13/multiprocessing/popen_spawn_posix.py:62\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.sentinel = parent_r\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     64\u001b[39m     fds_to_close = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==== Orthomosaic Testing Section ====\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, Features, Image, Value\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(115),\n",
    "    transforms.RandomCrop(77),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(77),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Transformation functions\n",
    "def transform_train_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "def transform_val_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        val_transforms(image.convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading dataset...\")\n",
    "original_dataset = load_dataset(\"mpg-ranch/horse-detection\", split=\"train\")\n",
    "df = pd.DataFrame(original_dataset)\n",
    "\n",
    "# Get list of all orthomosaics\n",
    "orthomosaics = list(df['orthomosaic'].unique())\n",
    "print(f\"Found {len(orthomosaics)} orthomosaics\")\n",
    "\n",
    "# Dictionary to store results\n",
    "orthomosaic_results = {}\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Train model for each orthomosaic\n",
    "for ortho in orthomosaics:\n",
    "    print(f\"\\nTraining model for orthomosaic: {ortho}\")\n",
    "    \n",
    "    # Filter data for this orthomosaic\n",
    "    ortho_df = df[df['orthomosaic'] == ortho]\n",
    "    \n",
    "    # Split into train and validation (80/20)\n",
    "    ortho_df = ortho_df.sample(frac=1, random_state=42)  # Shuffle\n",
    "    split_idx = int(0.8 * len(ortho_df))\n",
    "    train_ortho_df = ortho_df.iloc[:split_idx]\n",
    "    val_ortho_df = ortho_df.iloc[split_idx:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'image': train_ortho_df['image'].tolist(),\n",
    "        'Presence': train_ortho_df['Presence'].tolist()\n",
    "    }, features=Features({\n",
    "        'image': Image(),\n",
    "        'Presence': Value('int64')\n",
    "    }))\n",
    "    \n",
    "    val_dataset = Dataset.from_dict({\n",
    "        'image': val_ortho_df['image'].tolist(),\n",
    "        'Presence': val_ortho_df['Presence'].tolist()\n",
    "    }, features=Features({\n",
    "        'image': Image(),\n",
    "        'Presence': Value('int64')\n",
    "    }))\n",
    "    \n",
    "    # Apply transformations\n",
    "    transformed_train_dataset = train_dataset.map(\n",
    "        transform_train_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=[\"image\"]\n",
    "    )\n",
    "    \n",
    "    transformed_val_dataset = val_dataset.map(\n",
    "        transform_val_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=[\"image\"]\n",
    "    )\n",
    "    \n",
    "    # Set format for PyTorch\n",
    "    transformed_train_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])\n",
    "    transformed_val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        transformed_train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        transformed_val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            presences = batch['Presence'].float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, presences)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            all_train_preds.extend(predicted.cpu().numpy())\n",
    "            all_train_labels.extend(presences.cpu().numpy())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_val_preds = []\n",
    "        all_val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                images = batch['pixel_values'].to(device)\n",
    "                presences = batch['Presence'].float().to(device)\n",
    "                \n",
    "                outputs = model(images).squeeze(1)\n",
    "                loss = criterion(outputs, presences)\n",
    "                val_running_loss += loss.item()\n",
    "                \n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_labels.extend(presences.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        val_loss = val_running_loss / len(val_dataloader)\n",
    "        train_f1 = f1_score(all_train_labels, all_train_preds, average='binary')\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='binary')\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        # Update best F1\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    orthomosaic_results[ortho] = {\n",
    "        'best_val_f1': best_f1,\n",
    "        'num_train_samples': len(train_dataset),\n",
    "        'num_val_samples': len(val_dataset),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_f1s': train_f1s,\n",
    "        'val_f1s': val_f1s\n",
    "    }\n",
    "    \n",
    "    print(f\"Completed training for {ortho}\")\n",
    "    print(f\"Best validation F1: {best_f1:.4f}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create histogram of F1 scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "f1_scores = [results['best_val_f1'] for results in orthomosaic_results.values()]\n",
    "\n",
    "# Create histogram\n",
    "sns.histplot(f1_scores, bins=20, kde=True)\n",
    "plt.axvline(np.mean(f1_scores), color='r', linestyle='--', label=f'Mean: {np.mean(f1_scores):.3f}')\n",
    "plt.axvline(np.median(f1_scores), color='g', linestyle='--', label=f'Median: {np.median(f1_scores):.3f}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Best Validation F1 Scores Across Orthomosaics')\n",
    "plt.legend()\n",
    "\n",
    "# Add text with statistics\n",
    "stats_text = f\"\"\"\n",
    "Statistics:\n",
    "Mean: {np.mean(f1_scores):.3f}\n",
    "Median: {np.median(f1_scores):.3f}\n",
    "Std: {np.std(f1_scores):.3f}\n",
    "Min: {np.min(f1_scores):.3f}\n",
    "Max: {np.max(f1_scores):.3f}\n",
    "\"\"\"\n",
    "plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, \n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('../results/figures', exist_ok=True)\n",
    "print(\"Created or verified results/figures directory\")\n",
    "plt.savefig('../results/figures/f1_score_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary of Orthomosaic Models:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Orthomosaic':<20} {'Val F1':<10} {'Train Samples':<15} {'Val Samples':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for ortho, results in orthomosaic_results.items():\n",
    "    print(f\"{ortho:<20} {results['best_val_f1']:.4f} {results['num_train_samples']:<15} {results['num_val_samples']:<15}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpg-horses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
