{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Fundamentals\n",
    "\n",
    "This notebook covers the fundamentals of working with PyTorch for deep learning, including:\n",
    "- Loading data from Hugging Face\n",
    "- Applying proper transformations and normalization\n",
    "- Using pre-trained models (ResNet50)\n",
    "- Setting up training with appropriate optimization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for or install all necessary packages with conda from environment.yml\n",
    "# %conda env update -f ../environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Features, Value\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features explicitly to match the actual dataset schema\n",
    "features = Features({\n",
    "    'image': Value(dtype='binary'),  # Changed from 'string' to 'binary'\n",
    "    'idx': Value(dtype='int64'),\n",
    "    'Presence': Value(dtype='int64'),\n",
    "    'Zone': Value(dtype='int64'),\n",
    "    'Period': Value(dtype='int64'),\n",
    "    'Recency': Value(dtype='int64'),\n",
    "    'datetime_groundtruth': Value(dtype='timestamp[ns, tz=UTC]'),  # Changed from string to timestamp\n",
    "    'datetime_aerialsurvey': Value(dtype='timestamp[ns, tz=UTC]'),  # Changed from string to timestamp\n",
    "    'observation_offset': Value(dtype='int64'),\n",
    "    'Latitude': Value(dtype='float64'),\n",
    "    'Longitude': Value(dtype='float64'),\n",
    "    'Easting': Value(dtype='float64'),\n",
    "    'Northing': Value(dtype='float64'),\n",
    "    'Ellipsoidal_height': Value(dtype='float64'),\n",
    "    'orthomosaic': Value(dtype='string'),\n",
    "    '__index_level_0__': Value(dtype='int64')  # Added missing index column\n",
    "})\n",
    "\n",
    "# Load the training split\n",
    "train_dataset = load_dataset(\n",
    "    \"mpg-ranch/horse-detection\", \n",
    "    split=\"train\",  # Explicitly load the train split\n",
    "    token=os.environ.get(\"HUGGINGFACE_TOKEN\", None),\n",
    "    streaming=False,  # Set to False to enable map operation\n",
    "    features=features\n",
    ")\n",
    "\n",
    "# Load the test split\n",
    "test_dataset = load_dataset(\n",
    "    \"mpg-ranch/horse-detection\", \n",
    "    split=\"test\",  # Explicitly load the test split\n",
    "    token=os.environ.get(\"HUGGINGFACE_TOKEN\", None),\n",
    "    streaming=False,  # Set to False to enable map operation\n",
    "    features=features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access an example\n",
    "example = next(iter(train_dataset))\n",
    "\n",
    "image = Image.open(BytesIO(example[\"image\"]))\n",
    "image.show()\n",
    "\n",
    "print(f\"Presence: {example['Presence']} (1=horse present, 0=absent)\")\n",
    "print(f\"Image size: {image.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Transformations\n",
    "\n",
    "Data normalization is **critical** for model performance. We use ImageNet statistics for normalization since we'll be using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessing transforms directly from the weights\n",
    "weights = ResNet50_Weights.IMAGENET1K_V1\n",
    "val_transforms = weights.transforms()\n",
    "\n",
    "# Define transformations for training data (augmentations + the model's preprocessing)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Print available metadata keys\n",
    "print(\"Available metadata keys:\")\n",
    "if hasattr(weights, 'meta'):\n",
    "    print(list(weights.meta.keys()))\n",
    "else:\n",
    "    print(\"No 'meta' attribute found\")\n",
    "\n",
    "# Print the preprocessing information safely\n",
    "print(\"\\nModel expects the following preprocessing:\")\n",
    "try:\n",
    "    # Try different possible key names for input size\n",
    "    if hasattr(weights, 'meta'):\n",
    "        if 'input_size' in weights.meta:\n",
    "            print(f\"- Input size: {weights.meta['input_size']}\")\n",
    "        elif 'imageSize' in weights.meta:\n",
    "            print(f\"- Input size: {weights.meta['imageSize']}\")\n",
    "        else:\n",
    "            print(\"- Input size: Not found in metadata\")\n",
    "            \n",
    "        # Try different possible key names for mean/std\n",
    "        if 'mean' in weights.meta:\n",
    "            print(f\"- Mean: {weights.meta['mean']}\")\n",
    "        else:\n",
    "            print(\"- Mean: [0.485, 0.456, 0.406] (ImageNet standard)\")\n",
    "            \n",
    "        if 'std' in weights.meta:\n",
    "            print(f\"- Std: {weights.meta['std']}\")\n",
    "        else:\n",
    "            print(\"- Std: [0.229, 0.224, 0.225] (ImageNet standard)\")\n",
    "    else:\n",
    "        print(\"Metadata not available, using standard ImageNet values:\")\n",
    "        print(\"- Input size: [3, 224, 224]\")\n",
    "        print(\"- Mean: [0.485, 0.456, 0.406]\")\n",
    "        print(\"- Std: [0.229, 0.224, 0.225]\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing metadata: {e}\")\n",
    "    print(\"Using standard ImageNet values:\")\n",
    "    print(\"- Input size: [3, 224, 224]\")\n",
    "    print(\"- Mean: [0.485, 0.456, 0.406]\")\n",
    "    print(\"- Std: [0.229, 0.224, 0.225]\")\n",
    "\n",
    "# Print information about the transforms\n",
    "print(\"\\nTransforms from weights:\")\n",
    "print(val_transforms)\n",
    "\n",
    "# Validation transformations (no augmentation, only resize and normalize)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Transformations to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply transformations to the training dataset\n",
    "def transform_train_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        train_transforms(Image.open(BytesIO(image)).convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Function to apply transformations to the validation dataset\n",
    "def transform_val_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        val_transforms(Image.open(BytesIO(image)).convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# # Load the training split\n",
    "# train_dataset = load_dataset(\n",
    "#     \"mpg-ranch/horse-detection\", \n",
    "#     split=\"train\",  # Explicitly load the train split\n",
    "#     token=os.environ.get(\"HUGGINGFACE_TOKEN\", None),\n",
    "#     streaming=False,  # Set to False to enable map operation\n",
    "#     features=features\n",
    "# )\n",
    "\n",
    "# # Load the test split\n",
    "# test_dataset = load_dataset(\n",
    "#     \"mpg-ranch/horse-detection\", \n",
    "#     split=\"test\",  # Explicitly load the test split\n",
    "#     token=os.environ.get(\"HUGGINGFACE_TOKEN\", None),\n",
    "#     streaming=False,  # Set to False to enable map operation\n",
    "#     features=features\n",
    "# )\n",
    "\n",
    "# Apply transformations to training set\n",
    "transformed_train_dataset = train_dataset.map(\n",
    "    transform_train_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\"]  # Remove original images after transformation\n",
    ")\n",
    "\n",
    "# Apply transformations to test set\n",
    "transformed_val_dataset = test_dataset.map(\n",
    "    transform_val_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\"]\n",
    ")\n",
    "\n",
    "# Set the format for PyTorch\n",
    "transformed_train_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])\n",
    "transformed_val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    transformed_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    transformed_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Batch\n",
    "\n",
    "Let's visualize some images from our dataloader to verify transformations are applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize images for visualization\n",
    "def denormalize(tensor):\n",
    "    # Make sure tensor is the right shape and type\n",
    "    if tensor.ndim != 3:  # If not a single image with 3 dimensions (C,H,W)\n",
    "        if tensor.ndim == 4:  # If it's a batch of images (B,C,H,W)\n",
    "            tensor = tensor[0]  # Take the first image\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected tensor shape: {tensor.shape}\")\n",
    "    \n",
    "    # Make sure we're working with the image tensor, not other data\n",
    "    if tensor.shape[0] == 3:  # If first dimension is 3, it's likely the channel dimension\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        return tensor * std + mean\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 3 channels, got {tensor.shape[0]}\")\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# Check the structure of the batch\n",
    "print(f\"Batch type: {type(batch)}\")\n",
    "if isinstance(batch, dict):\n",
    "    print(f\"Batch keys: {batch.keys()}\")\n",
    "    images = batch['pixel_values']  # Adjust based on your actual key\n",
    "    Presences = batch['Presence']  # Adjust based on your actual key\n",
    "else:\n",
    "    # If it's a tuple or list, unpack accordingly\n",
    "    images, Presences = batch\n",
    "\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Presences shape: {Presences.shape}\")\n",
    "\n",
    "# Visualize a few images from the batch\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(images):\n",
    "        # Denormalize the image\n",
    "        img = denormalize(images[i])\n",
    "        img = img.permute(1, 2, 0).numpy()  # Change from CxHxW to HxWxC\n",
    "        img = np.clip(img, 0, 1)  # Clip values to valid range\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Presence: {Presences[i].item()}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final fully connected layer for our task\n",
    "# For CIFAR-10, we have 10 classes\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Print model architecture summary\n",
    "print(f\"Model: ResNet50\")\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.fc.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "We'll use default settings for the optimizer and only tune the learning rate and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer with default settings\n",
    "# Only optimize the parameters of the new head (model.fc)\n",
    "learning_rate = 0.001  # This is one of the few hyperparameters we'll tune\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "# Number of epochs is another parameter we'll tune\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "# Check the structure of a batch to understand the data format\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"Batch type: {type(sample_batch)}\")\n",
    "if isinstance(sample_batch, dict):\n",
    "    print(f\"Batch keys: {sample_batch.keys()}\")\n",
    "    # Adjust these based on your actual keys\n",
    "    image_key = 'pixel_values' if 'pixel_values' in sample_batch else 'img'\n",
    "    Presence_key = 'Presence' if 'Presence' in sample_batch else 'Presences'\n",
    "    print(f\"Using keys - Images: '{image_key}', Presences: '{Presence_key}'\")\n",
    "else:\n",
    "    print(f\"Batch is a {type(sample_batch)} with {len(sample_batch)} elements\")\n",
    "    for i, item in enumerate(sample_batch):\n",
    "        print(f\"  Item {i} type: {type(item)}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        # Extract images and Presences based on the batch structure\n",
    "        if isinstance(batch, dict):\n",
    "            # Dictionary format (common with Hugging Face datasets)\n",
    "            images = batch['pixel_values'] if 'pixel_values' in batch else batch['img']\n",
    "            Presences = batch['Presence'] if 'Presence' in batch else batch['Presences']\n",
    "        else:\n",
    "            # Tuple/list format (common with PyTorch datasets)\n",
    "            images, Presences = batch\n",
    "        \n",
    "        # Move data to device\n",
    "        images, Presences = images.to(device), Presences.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, Presences)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += Presences.size(0)\n",
    "        correct += predicted.eq(Presences).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # Extract images and Presences based on the batch structure\n",
    "            if isinstance(batch, dict):\n",
    "                # Dictionary format (common with Hugging Face datasets)\n",
    "                images = batch['pixel_values'] if 'pixel_values' in batch else batch['img']\n",
    "                Presences = batch['Presence'] if 'Presence' in batch else batch['Presences']\n",
    "            else:\n",
    "                # Tuple/list format (common with PyTorch datasets)\n",
    "                images, Presences = batch\n",
    "            \n",
    "            # Move data to device\n",
    "            images, Presences = images.to(device), Presences.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, Presences)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += Presences.size(0)\n",
    "            correct += predicted.eq(Presences).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss / len(val_dataloader)\n",
    "    val_acc = 100. * correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation metrics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "ax1.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(range(1, num_epochs+1), train_accs, label='Train Accuracy')\n",
    "ax2.plot(range(1, num_epochs+1), val_accs, label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, Presences in val_dataloader:\n",
    "        images, Presences = images.to(device), Presences.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += Presences.size(0)\n",
    "        correct += (predicted == Presences).sum().item()\n",
    "        \n",
    "        # Calculate accuracy for each class\n",
    "        c = (predicted == Presences).squeeze()\n",
    "        for i in range(Presences.size(0)):\n",
    "            Presence = Presences[i]\n",
    "            class_correct[Presence] += c[i].item()\n",
    "            class_total[Presence] += 1\n",
    "\n",
    "print(f'Overall Accuracy on the test set: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Print accuracy for each class\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "for i in range(10):\n",
    "    print(f'Accuracy of {classes[i]}: {100 * class_correct[i] / class_total[i]:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points to Remember\n",
    "\n",
    "1. **Data Normalization is Critical**\n",
    "   - Always normalize your input data using mean and standard deviation\n",
    "   - For transfer learning with pre-trained models, use the same normalization values that were used during pre-training (e.g., ImageNet stats)\n",
    "\n",
    "2. **Data Transformations**\n",
    "   - Apply appropriate augmentations for training data (flips, rotations, crops)\n",
    "   - Use only resizing and normalization for validation/test data\n",
    "   - Transformations help prevent overfitting and improve model generalization\n",
    "\n",
    "3. **Model Architecture**\n",
    "   - Start with a pre-trained model like ResNet50\n",
    "   - Modify only the final layer (head) to match your specific task\n",
    "   - Freeze pre-trained layers initially to leverage transfer learning\n",
    "\n",
    "4. **Optimization Settings**\n",
    "   - Start with default optimizer settings\n",
    "   - Focus on tuning learning rate and number of epochs first\n",
    "   - Monitor validation metrics to prevent overfitting\n",
    "\n",
    "5. **Progressive Unfreezing**\n",
    "   - After initial training, you can unfreeze more layers gradually\n",
    "   - Use a smaller learning rate when fine-tuning pre-trained layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Hyperparameter Tuning**\n",
    "   - Try different learning rates\n",
    "   - Experiment with different batch sizes\n",
    "   - Test different optimizers (SGD with momentum, AdamW)\n",
    "\n",
    "2. **Model Improvements**\n",
    "   - Unfreeze more layers for fine-tuning\n",
    "   - Try different pre-trained architectures (EfficientNet, ViT)\n",
    "   - Implement learning rate scheduling\n",
    "\n",
    "3. **Advanced Techniques**\n",
    "   - Implement data augmentation strategies like mixup or cutmix\n",
    "   - Try different loss functions\n",
    "   - Implement ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpg-horses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
