{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Fundamentals\n",
    "\n",
    "This notebook covers the fundamentals of working with PyTorch for deep learning, including:\n",
    "- Loading data from Hugging Face\n",
    "- Applying proper transformations and normalization\n",
    "- Using pre-trained models (ResNet50)\n",
    "- Setting up training with appropriate optimization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for or install all necessary packages with conda from environment.yml\n",
    "# %conda env update -f ../environment.yml\n",
    "\n",
    "# For colab\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset, Features, Value\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "# Import F1 score calculation\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face with token if necessary\n",
    "# login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"mpg-ranch/horse-detection\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation split from the training data\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"image\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preprocessing Requirements\n",
    "\n",
    "First, let's examine what preprocessing is expected by the pretrained ResNet50 model. We'll check the metadata from the model weights to understand the expected input size, normalization values, and default transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessing transforms directly from the weights\n",
    "weights = ResNet50_Weights.IMAGENET1K_V1\n",
    "default_transforms = weights.transforms()\n",
    "\n",
    "# Print available metadata keys\n",
    "print(\"Available metadata keys:\")\n",
    "if hasattr(weights, 'meta'):\n",
    "    print(list(weights.meta.keys()))\n",
    "else:\n",
    "    print(\"No 'meta' attribute found\")\n",
    "\n",
    "# Print the preprocessing information safely\n",
    "print(\"\\nModel expects the following preprocessing:\")\n",
    "try:\n",
    "    # Try different possible key names for input size\n",
    "    if hasattr(weights, 'meta'):\n",
    "        if 'input_size' in weights.meta:\n",
    "            print(f\"- Input size: {weights.meta['input_size']}\")\n",
    "        elif 'imageSize' in weights.meta:\n",
    "            print(f\"- Input size: {weights.meta['imageSize']}\")\n",
    "        else:\n",
    "            print(\"- Input size: Not found in metadata\")\n",
    "            \n",
    "        # Try different possible key names for mean/std\n",
    "        if 'mean' in weights.meta:\n",
    "            print(f\"- Mean: {weights.meta['mean']}\")\n",
    "        else:\n",
    "            print(\"- Mean: [0.485, 0.456, 0.406] (ImageNet standard)\")\n",
    "            \n",
    "        if 'std' in weights.meta:\n",
    "            print(f\"- Std: {weights.meta['std']}\")\n",
    "        else:\n",
    "            print(\"- Std: [0.229, 0.224, 0.225] (ImageNet standard)\")\n",
    "    else:\n",
    "        print(\"Metadata not available, using standard ImageNet values:\")\n",
    "        print(\"- Input size: [3, 224, 224]\")\n",
    "        print(\"- Mean: [0.485, 0.456, 0.406]\")\n",
    "        print(\"- Std: [0.229, 0.224, 0.225]\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing metadata: {e}\")\n",
    "    print(\"Using standard ImageNet values:\")\n",
    "    print(\"- Input size: [3, 224, 224]\")\n",
    "    print(\"- Mean: [0.485, 0.456, 0.406]\")\n",
    "    print(\"- Std: [0.229, 0.224, 0.225]\")\n",
    "\n",
    "# Print information about the default transforms\n",
    "print(\"\\nDefault transforms from weights:\")\n",
    "print(default_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transforms for Our Dataset\n",
    "\n",
    "Based on the model requirements, we'll now define our custom transforms for both training and validation datasets:\n",
    "\n",
    "1. **Training transforms**: Include data augmentation (random crops, flips, rotation) to improve model generalization, along with the required normalization\n",
    "2. **Validation transforms**: Only include deterministic center cropping and normalization (no augmentation) to evaluate the model on consistent inputs\n",
    "\n",
    "Note the specific cropping approach:\n",
    "- Training: 115px center crop followed by a 77px random crop to simulate our camera setup and add variation\n",
    "- Validation: 77px deterministic center crop (1m equivalent in our camera setup) for consistent evaluation\n",
    "\n",
    "Both transforms include the same normalization parameters to match the pretrained model's expectations, but validation excludes augmentation to ensure consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training data (augmentations + the model's preprocessing)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(115),  # First, center crop to 1.5m (115 pixels)\n",
    "    transforms.RandomCrop(77),   # Then, random crop to 1m (77 pixels)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Validation transformations (no augmentation, only resize and normalize)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(77),   # Deterministic crop to 1m (77 pixels)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Print your custom transforms\n",
    "print(\"Custom transforms defined for training and validation:\")\n",
    "print(\"\\nTraining transforms:\")\n",
    "print(train_transforms)\n",
    "print(\"\\nValidation transforms:\")\n",
    "print(val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Transformations to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply transformations to the training dataset\n",
    "def transform_train_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Function to apply transformations to the validation dataset\n",
    "def transform_val_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        val_transforms(image.convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Apply transformations to training set\n",
    "transformed_train_dataset = train_dataset.map(\n",
    "    transform_train_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\"]  # Remove original images after transformation\n",
    ")\n",
    "\n",
    "# Apply transformations to test set\n",
    "transformed_val_dataset = val_dataset.map(\n",
    "    transform_val_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\"]\n",
    ")\n",
    "\n",
    "# Set the format for PyTorch\n",
    "transformed_train_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])\n",
    "transformed_val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    transformed_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    transformed_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Batch\n",
    "\n",
    "Let's visualize some images from our dataloader to verify transformations are applied correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize images for visualization\n",
    "def denormalize(tensor):\n",
    "    # Make sure tensor is the right shape and type\n",
    "    if tensor.ndim != 3:  # If not a single image with 3 dimensions (C,H,W)\n",
    "        if tensor.ndim == 4:  # If it's a batch of images (B,C,H,W)\n",
    "            tensor = tensor[0]  # Take the first image\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected tensor shape: {tensor.shape}\")\n",
    "    \n",
    "    # Make sure we're working with the image tensor, not other data\n",
    "    if tensor.shape[0] == 3:  # If first dimension is 3, it's likely the channel dimension\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        return tensor * std + mean\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 3 channels, got {tensor.shape[0]}\")\n",
    "\n",
    "# Get a batch from the dataloader\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# Check the structure of the batch\n",
    "print(f\"Batch type: {type(batch)}\")\n",
    "if isinstance(batch, dict):\n",
    "    print(f\"Batch keys: {batch.keys()}\")\n",
    "    images = batch['pixel_values']  # Adjust based on your actual key\n",
    "    Presences = batch['Presence']  # Adjust based on your actual key\n",
    "else:\n",
    "    # If it's a tuple or list, unpack accordingly\n",
    "    images, Presences = batch\n",
    "\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Presences shape: {Presences.shape}\")\n",
    "\n",
    "# Visualize a few images from the batch\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < len(images):\n",
    "        # Denormalize the image\n",
    "        img = denormalize(images[i])\n",
    "        img = img.permute(1, 2, 0).numpy()  # Change from CxHxW to HxWxC\n",
    "        img = np.clip(img, 0, 1)  # Clip values to valid range\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Presence: {Presences[i].item()}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Unfreeze all layers from the start\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Modify the final fully connected layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  # Output a single value\n",
    "\n",
    "# Print model architecture summary\n",
    "print(f\"Model: ResNet50\")\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "We'll use default settings for the optimizer and only tune the learning rate and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function for binary classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Use a smaller learning rate for fine-tuning all layers\n",
    "learning_rate = 0.0001  # Reduced from 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Number of epochs is another parameter we'll tune\n",
    "num_epochs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rates to test\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# Function to train model with a specific learning rate\n",
    "def train_with_learning_rate(lr, num_epochs=3):\n",
    "    # Initialize a fresh model (using the same architecture)\n",
    "    test_model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Unfreeze all layers\n",
    "    for param in test_model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Modify the final fully connected layer for binary classification\n",
    "    test_model.fc = nn.Linear(test_model.fc.in_features, 1)\n",
    "    test_model = test_model.to(device)\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Use the specified learning rate\n",
    "    optimizer = optim.Adam(test_model.parameters(), lr=lr)\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    val_f1s = []\n",
    "    \n",
    "    # Run for fewer epochs when testing learning rates\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        test_model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            # Extract data\n",
    "            if isinstance(batch, dict):\n",
    "                images = batch['pixel_values']\n",
    "                Presences = batch['Presence']\n",
    "            else:\n",
    "                images, Presences = batch\n",
    "            \n",
    "            # Move to device and prepare\n",
    "            images, Presences = images.to(device), Presences.to(device)\n",
    "            Presences = Presences.float()\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = test_model(images)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = criterion(outputs, Presences)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        test_model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                # Extract data\n",
    "                if isinstance(batch, dict):\n",
    "                    images = batch['pixel_values']\n",
    "                    Presences = batch['Presence']\n",
    "                else:\n",
    "                    images, Presences = batch\n",
    "                \n",
    "                # Move to device and prepare\n",
    "                images, Presences = images.to(device), Presences.to(device)\n",
    "                Presences = Presences.float()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = test_model(images)\n",
    "                outputs = outputs.squeeze(1)\n",
    "                loss = criterion(outputs, Presences)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                val_running_loss += loss.item()\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                total += Presences.size(0)\n",
    "                correct += predicted.eq(Presences).sum().item()\n",
    "                \n",
    "                # Store predictions for F1 score\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(Presences.cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_loss = val_running_loss / len(val_dataloader)\n",
    "        val_acc = 100. * correct / total\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "        \n",
    "        # Store metrics\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'LR: {lr:.6f}, Epoch: {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "              f'Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'val_f1s': val_f1s,\n",
    "        'final_val_acc': val_accs[-1],\n",
    "        'final_val_f1': val_f1s[-1]\n",
    "    }\n",
    "\n",
    "# Set device (this should match the one used in the main training loop)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dictionary to store results\n",
    "lr_results = {}\n",
    "\n",
    "# Test each learning rate\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\nTesting learning rate: {lr}\")\n",
    "    lr_results[lr] = train_with_learning_rate(lr)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(2, 2, 1)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(range(1, len(lr_results[lr]['train_losses'])+1), \n",
    "             lr_results[lr]['train_losses'], \n",
    "             label=f'LR: {lr}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss by Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation loss\n",
    "plt.subplot(2, 2, 2)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(range(1, len(lr_results[lr]['val_losses'])+1), \n",
    "             lr_results[lr]['val_losses'], \n",
    "             label=f'LR: {lr}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss by Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.subplot(2, 2, 3)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(range(1, len(lr_results[lr]['val_accs'])+1), \n",
    "             lr_results[lr]['val_accs'], \n",
    "             label=f'LR: {lr}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Validation Accuracy by Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation F1 score\n",
    "plt.subplot(2, 2, 4)\n",
    "for lr in learning_rates:\n",
    "    plt.plot(range(1, len(lr_results[lr]['val_f1s'])+1), \n",
    "             lr_results[lr]['val_f1s'], \n",
    "             label=f'LR: {lr}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation F1 Score')\n",
    "plt.title('Validation F1 Score by Learning Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Learning Rate Performance:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Learning Rate':<15} {'Final Val Acc':<15} {'Final Val F1':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for lr in learning_rates:\n",
    "    print(f\"{lr:<15.6f} {lr_results[lr]['final_val_acc']:<15.2f} {lr_results[lr]['final_val_f1']:<15.4f}\")\n",
    "\n",
    "# Identify best learning rate based on validation accuracy\n",
    "best_lr = max(learning_rates, key=lambda lr: lr_results[lr]['final_val_acc'])\n",
    "print(f\"\\nBest learning rate based on validation accuracy: {best_lr}\")\n",
    "\n",
    "# Recommended learning rate to use in the main training loop\n",
    "print(f\"\\nRecommendation: Use learning_rate = {best_lr} in the main training configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "train_f1s = []  # New list for training F1 scores\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "val_f1s = []    # New list for validation F1 scores\n",
    "\n",
    "# Check the structure of a batch to understand the data format\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"Batch type: {type(sample_batch)}\")\n",
    "if isinstance(sample_batch, dict):\n",
    "    print(f\"Batch keys: {sample_batch.keys()}\")\n",
    "    # Adjust these based on your actual keys\n",
    "    image_key = 'pixel_values' if 'pixel_values' in sample_batch else 'img'\n",
    "    Presence_key = 'Presence' if 'Presence' in sample_batch else 'Presences'\n",
    "    print(f\"Using keys - Images: '{image_key}', Presences: '{Presence_key}'\")\n",
    "else:\n",
    "    print(f\"Batch is a {type(sample_batch)} with {len(sample_batch)} elements\")\n",
    "    for i, item in enumerate(sample_batch):\n",
    "        print(f\"  Item {i} type: {type(item)}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_train_preds = []  # To store all training predictions\n",
    "    all_train_labels = [] # To store all training labels\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Extract images and Presences based on the batch structure\n",
    "        if isinstance(batch, dict):\n",
    "            # Dictionary format (common with Hugging Face datasets)\n",
    "            images = batch['pixel_values'] if 'pixel_values' in batch else batch['img']\n",
    "            Presences = batch['Presence'] if 'Presence' in batch else batch['Presences']\n",
    "        else:\n",
    "            # Tuple/list format (common with PyTorch datasets)\n",
    "            images, Presences = batch\n",
    "\n",
    "        # Move data to device\n",
    "        images, Presences = images.to(device), Presences.to(device)\n",
    "\n",
    "        # Convert Presences to float for BCEWithLogitsLoss\n",
    "        Presences = Presences.float()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.squeeze(1)  # Change from [batch_size, 1] to [batch_size]\n",
    "        loss = criterion(outputs, Presences)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # For binary classification with BCEWithLogitsLoss\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total += Presences.size(0)\n",
    "        correct += predicted.eq(Presences).sum().item()\n",
    "\n",
    "        # Collect predictions and labels for F1 calculation\n",
    "        all_train_preds.extend(predicted.cpu().numpy())\n",
    "        all_train_labels.extend(Presences.cpu().numpy())\n",
    "\n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_f1 = f1_score(all_train_labels, all_train_preds, average='binary')\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_f1s.append(train_f1)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_val_preds = []    # To store all validation predictions\n",
    "    all_val_labels = []   # To store all validation labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            # Extract images and Presences based on the batch structure\n",
    "            if isinstance(batch, dict):\n",
    "                # Dictionary format (common with Hugging Face datasets)\n",
    "                images = batch['pixel_values'] if 'pixel_values' in batch else batch['img']\n",
    "                Presences = batch['Presence'] if 'Presence' in batch else batch['Presences']\n",
    "            else:\n",
    "                # Tuple/list format (common with PyTorch datasets)\n",
    "                images, Presences = batch\n",
    "\n",
    "            # Move data to device\n",
    "            images, Presences = images.to(device), Presences.to(device)\n",
    "\n",
    "            # Convert Presences to float for BCEWithLogitsLoss\n",
    "            Presences = Presences.float()\n",
    "\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.squeeze(1)  # Change from [batch_size, 1] to [batch_size]\n",
    "            loss = criterion(outputs, Presences)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            # For binary classification with BCEWithLogitsLoss\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += Presences.size(0)\n",
    "            correct += predicted.eq(Presences).sum().item()\n",
    "\n",
    "            # Collect predictions and labels for F1 calculation\n",
    "            all_val_preds.extend(predicted.cpu().numpy())\n",
    "            all_val_labels.extend(Presences.cpu().numpy())\n",
    "\n",
    "    # Calculate validation metrics\n",
    "    val_loss = val_running_loss / len(val_dataloader)\n",
    "    val_acc = 100. * correct / total\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='binary')\n",
    "\n",
    "    # Store metrics\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    # Print epoch results including F1 scores\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Train F1: {train_f1:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation metrics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "ax1.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(range(1, num_epochs+1), train_accs, label='Train Accuracy')\n",
    "ax2.plot(range(1, num_epochs+1), val_accs, label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points to Remember\n",
    "\n",
    "1. **Data Normalization is Critical**\n",
    "   - Always normalize your input data using mean and standard deviation\n",
    "   - For transfer learning with pre-trained models, use the same normalization values that were used during pre-training (e.g., ImageNet stats)\n",
    "\n",
    "2. **Data Transformations**\n",
    "   - Apply appropriate augmentations for training data (flips, rotations, crops)\n",
    "   - Use only resizing and normalization for validation/test data\n",
    "   - Transformations help prevent overfitting and improve model generalization\n",
    "\n",
    "3. **Model Architecture**\n",
    "   - Start with a pre-trained model like ResNet50\n",
    "   - Modify only the final layer (head) to match your specific task\n",
    "   - Freeze pre-trained layers initially to leverage transfer learning\n",
    "\n",
    "4. **Optimization Settings**\n",
    "   - Start with default optimizer settings\n",
    "   - Focus on tuning learning rate and number of epochs first\n",
    "   - Monitor validation metrics to prevent overfitting\n",
    "\n",
    "5. **Progressive Unfreezing**\n",
    "   - After initial training, you can unfreeze more layers gradually\n",
    "   - Use a smaller learning rate when fine-tuning pre-trained layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Hyperparameter Tuning**\n",
    "   - Try different learning rates\n",
    "   - Experiment with different batch sizes\n",
    "   - Test different optimizers (SGD with momentum, AdamW)\n",
    "\n",
    "2. **Model Improvements**\n",
    "   - Unfreeze more layers for fine-tuning\n",
    "   - Try different pre-trained architectures (EfficientNet, ViT)\n",
    "   - Implement learning rate scheduling\n",
    "\n",
    "3. **Advanced Techniques**\n",
    "   - Implement data augmentation strategies like mixup or cutmix\n",
    "   - Try different loss functions\n",
    "   - Implement ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthomosaic Testing Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Orthomosaic-Aware Train-Test Split ====\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load original dataset with orthomosaic information\n",
    "original_dataset = load_dataset(\"mpg-ranch/horse-detection\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for efficient grouping\n",
    "df = pd.DataFrame(original_dataset)\n",
    "\n",
    "# Group by orthomosaic using pandas groupby\n",
    "orthomosaic_groups = {\n",
    "    ortho: group \n",
    "    for ortho, group in df.groupby('orthomosaic')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split images within each orthomosaic group\n",
    "train_examples = []\n",
    "val_examples = []\n",
    "\n",
    "for ortho, group_df in orthomosaic_groups.items():\n",
    "    # Shuffle the group\n",
    "    group_df = group_df.sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Calculate split index for this orthomosaic\n",
    "    split_idx = int(0.8 * len(group_df))\n",
    "    \n",
    "    # Split into train and validation\n",
    "    train_group = group_df.iloc[:split_idx]\n",
    "    val_group = group_df.iloc[split_idx:]\n",
    "    \n",
    "    # Add to respective lists\n",
    "    train_examples.extend(train_group.to_dict('records'))\n",
    "    val_examples.extend(val_group.to_dict('records'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datasets\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "val_dataset = Dataset.from_list(val_examples)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total orthomosaics: {len(orthomosaic_groups)}\")\n",
    "print(f\"Training set: {len(train_dataset)} examples\")\n",
    "print(f\"Validation set: {len(val_dataset)} examples\")\n",
    "\n",
    "# Print orthomosaic distribution\n",
    "print(\"\\nOrthomosaic distribution in training set:\")\n",
    "train_counts = pd.DataFrame(train_examples)['orthomosaic'].value_counts()\n",
    "for ortho, count in train_counts.items():\n",
    "    print(f\"- {ortho}: {count} examples\")\n",
    "\n",
    "print(\"\\nOrthomosaic distribution in validation set:\")\n",
    "val_counts = pd.DataFrame(val_examples)['orthomosaic'].value_counts()\n",
    "for ortho, count in val_counts.items():\n",
    "    print(f\"- {ortho}: {count} examples\")\n",
    "\n",
    "# Save the split information for future reference\n",
    "split_info = {\n",
    "    'total_orthomosaics': len(orthomosaic_groups),\n",
    "    'train_size': len(train_dataset),\n",
    "    'val_size': len(val_dataset),\n",
    "    'orthomosaic_distribution': {\n",
    "        'train': train_counts.to_dict(),\n",
    "        'val': val_counts.to_dict()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "with open('../results/split_info.json', 'w') as f:\n",
    "    json.dump(split_info, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Data Preprocessing and Transformations ====\n",
    "\n",
    "# Define transformations for training data (augmentations + the model's preprocessing)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(115),  # First, center crop to 1.5m (115 pixels)\n",
    "    transforms.RandomCrop(77),   # Then, random crop to 1m (77 pixels)\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Validation transformations (no augmentation, only resize and normalize)\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.CenterCrop(77),   # Deterministic crop to 1m (77 pixels)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Apply transformations to datasets\n",
    "def transform_train_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "def transform_val_dataset(examples):\n",
    "    examples[\"pixel_values\"] = [\n",
    "        val_transforms(image.convert(\"RGB\")) \n",
    "        for image in examples[\"image\"]\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Apply transformations\n",
    "transformed_train_dataset = train_dataset.map(\n",
    "    transform_train_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\"]\n",
    ")\n",
    "\n",
    "transformed_val_dataset = val_dataset.map(\n",
    "    transform_val_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=[\"image\"]\n",
    ")\n",
    "\n",
    "# Set the format for PyTorch\n",
    "transformed_train_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])\n",
    "transformed_val_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Create DataLoaders ====\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    transformed_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    transformed_val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Print dataloader information\n",
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Model Setup ====\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Unfreeze all layers from the start\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Modify the final fully connected layer for binary classification\n",
    "model.fc = nn.Linear(model.fc.in_features, 1)  # Output a single value\n",
    "\n",
    "# Print model architecture summary\n",
    "print(f\"Model: ResNet50\")\n",
    "print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Set learning rate and optimizer\n",
    "learning_rate = 0.0001  # Small learning rate for fine-tuning\n",
    "print(f\"Using learning rate: {learning_rate}\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Training Loop ====\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "train_f1s = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "val_f1s = []\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_train_preds = []\n",
    "    all_train_labels = []\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        images = batch['pixel_values'].to(device)\n",
    "        presences = batch['Presence'].float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze(1)\n",
    "        loss = criterion(outputs, presences)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        total += presences.size(0)\n",
    "        correct += predicted.eq(presences).sum().item()\n",
    "        \n",
    "        # Store predictions for F1 calculation\n",
    "        all_train_preds.extend(predicted.cpu().numpy())\n",
    "        all_train_labels.extend(presences.cpu().numpy())\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_loss = running_loss / len(train_dataloader)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_f1 = f1_score(all_train_labels, all_train_preds, average='binary')\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            presences = batch['Presence'].float().to(device)\n",
    "            \n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, presences)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += presences.size(0)\n",
    "            correct += predicted.eq(presences).sum().item()\n",
    "            \n",
    "            all_val_preds.extend(predicted.cpu().numpy())\n",
    "            all_val_labels.extend(presences.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    val_loss = val_running_loss / len(val_dataloader)\n",
    "    val_acc = 100. * correct / total\n",
    "    val_f1 = f1_score(all_val_labels, all_val_preds, average='binary')\n",
    "    \n",
    "    # Store metrics\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Train F1: {train_f1:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1: {val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Visualization of Training Progress ====\n",
    "\n",
    "# Plot training and validation metrics\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "ax1.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(range(1, num_epochs+1), train_accs, label='Train Accuracy')\n",
    "ax2.plot(range(1, num_epochs+1), val_accs, label='Validation Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Plot F1 score\n",
    "ax3.plot(range(1, num_epochs+1), train_f1s, label='Train F1')\n",
    "ax3.plot(range(1, num_epochs+1), val_f1s, label='Validation F1')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('F1 Score')\n",
    "ax3.set_title('Training and Validation F1 Score')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plots\n",
    "os.makedirs('../results/figures', exist_ok=True)\n",
    "plt.savefig('../results/figures/training_progress.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Orthomosaic-wise Performance Analysis ====\n",
    "\n",
    "# Evaluate model performance on each orthomosaic\n",
    "orthomosaic_results = {}\n",
    "model.eval()\n",
    "\n",
    "for ortho in orthomosaics:\n",
    "    # Filter dataset for this orthomosaic\n",
    "    filtered_dataset = Dataset.from_list(orthomosaic_groups[ortho])\n",
    "    \n",
    "    # Apply validation transforms\n",
    "    transformed_dataset = filtered_dataset.map(\n",
    "        transform_val_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=[\"image\"]\n",
    "    )\n",
    "    transformed_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"Presence\"])\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Evaluate\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['pixel_values'].to(device)\n",
    "            presences = batch['Presence'].float().to(device)\n",
    "            \n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, presences)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            total += presences.size(0)\n",
    "            correct += predicted.eq(presences).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(presences.cpu().numpy())\n",
    "    \n",
    "    if total > 0:\n",
    "        f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "        orthomosaic_results[ortho] = {\n",
    "            'f1_score': f1,\n",
    "            'accuracy': 100.0 * correct / total,\n",
    "            'num_samples': total,\n",
    "            'is_train': ortho in train_orthos\n",
    "        }\n",
    "        print(f\"{ortho}: F1={f1:.4f}, Acc={100.0*correct/total:.2f}%, n={total}, {'Train' if ortho in train_orthos else 'Val'}\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Bar chart of F1 scores\n",
    "sorted_orthos = sorted(orthomosaic_results.keys())\n",
    "sorted_f1s = [orthomosaic_results[ortho]['f1_score'] for ortho in sorted_orthos]\n",
    "colors = ['steelblue' if orthomosaic_results[ortho]['is_train'] else 'orange' for ortho in sorted_orthos]\n",
    "\n",
    "ax1.bar(range(len(sorted_orthos)), sorted_f1s, color=colors, alpha=0.7)\n",
    "ax1.set_xticks(range(len(sorted_orthos)))\n",
    "ax1.set_xticklabels(sorted_orthos, rotation=90)\n",
    "ax1.set_xlabel('Orthomosaic')\n",
    "ax1.set_ylabel('F1 Score')\n",
    "ax1.set_title('F1 Scores by Orthomosaic')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Scatter plot of F1 vs number of samples\n",
    "sizes = [orthomosaic_results[ortho]['num_samples'] for ortho in sorted_orthos]\n",
    "ax2.scatter(sizes, sorted_f1s, c=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Number of Samples')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.set_title('F1 Score vs Number of Samples')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/orthomosaic_analysis.png')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Mean F1 Score: {np.mean(sorted_f1s):.4f}\")\n",
    "print(f\"Std F1 Score: {np.std(sorted_f1s):.4f}\")\n",
    "print(f\"Min F1 Score: {min(sorted_f1s):.4f}\")\n",
    "print(f\"Max F1 Score: {max(sorted_f1s):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpg-horses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
