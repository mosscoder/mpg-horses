{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split for Horse Detection Dataset\n",
    "\n",
    "This notebook implements a train/test split for the horse detection dataset according to the following requirements:\n",
    "\n",
    "1. Reserve 20% of the data for final evaluation\n",
    "2. For absences: exclude any absence found within 2 meters of a presence (at any time point), then group by spatial blocks and sample 20% randomly from each group\n",
    "3. For presences: group by zone and period, sampling 20% randomly from each group\n",
    "4. Save each split in its own file and document in the Hugging Face markdown file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data\n",
    "groundtruth_path = '../data/vector/groundtruth.geojson'\n",
    "gdf = gpd.read_file(groundtruth_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Total points: {len(gdf)}\")\n",
    "print(f\"Presence points: {len(gdf[gdf['Presence'] == 1])}\")\n",
    "print(f\"Absence points: {len(gdf[gdf['Presence'] == 0])}\")\n",
    "print(f\"Unique zones: {gdf['Zone'].unique()}\")\n",
    "print(f\"Unique periods: {gdf['Period'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spatial Blocks\n",
    "\n",
    "We'll use K-means clustering to create 10 spatial blocks based on the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates for clustering\n",
    "coords = np.column_stack((gdf.geometry.x, gdf.geometry.y))\n",
    "\n",
    "# Create 10 spatial blocks using K-means clustering\n",
    "kmeans = KMeans(n_clusters=10, random_state=RANDOM_SEED)\n",
    "gdf['spatial_block'] = kmeans.fit_predict(coords)\n",
    "\n",
    "# Visualize the spatial blocks\n",
    "plt.figure(figsize=(12, 10))\n",
    "for block_id in range(10):\n",
    "    block_points = gdf[gdf['spatial_block'] == block_id]\n",
    "    plt.scatter(block_points.geometry.x, block_points.geometry.y, \n",
    "                label=f'Block {block_id}', alpha=0.7)\n",
    "\n",
    "plt.title('Spatial Blocks Created by K-means Clustering')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Count points in each spatial block\n",
    "block_counts = gdf.groupby('spatial_block').size()\n",
    "print(\"Points per spatial block:\")\n",
    "print(block_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Absences\n",
    "\n",
    "1. Exclude absences within 2 meters of any presence\n",
    "2. Group by spatial block\n",
    "3. Sample 20% from each group for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate presence and absence points\n",
    "presence_gdf = gdf[gdf['Presence'] == 1]\n",
    "absence_gdf = gdf[gdf['Presence'] == 0]\n",
    "\n",
    "print(f\"Original absence points: {len(absence_gdf)}\")\n",
    "\n",
    "# Create a 2-meter buffer around each presence point\n",
    "presence_buffers = presence_gdf.copy()\n",
    "presence_buffers.geometry = presence_gdf.geometry.buffer(2)\n",
    "\n",
    "# Find absences that are NOT within any presence buffer\n",
    "valid_absences = absence_gdf.copy()\n",
    "for idx, buffer in tqdm(presence_buffers.iterrows(), total=len(presence_buffers), desc=\"Filtering absences\"):\n",
    "    # Remove absences that intersect with this buffer\n",
    "    valid_absences = valid_absences[~valid_absences.geometry.intersects(buffer.geometry)]\n",
    "\n",
    "print(f\"Valid absence points (after removing those within 2m of presences): {len(valid_absences)}\")\n",
    "\n",
    "# Group valid absences by spatial block\n",
    "absence_test_indices = []\n",
    "absence_train_indices = []\n",
    "\n",
    "for block_id in valid_absences['spatial_block'].unique():\n",
    "    block_absences = valid_absences[valid_absences['spatial_block'] == block_id]\n",
    "    \n",
    "    # Sample 20% for testing\n",
    "    test_size = max(1, int(0.2 * len(block_absences)))\n",
    "    test_indices = block_absences.sample(n=test_size, random_state=RANDOM_SEED).index.tolist()\n",
    "    train_indices = block_absences.index.difference(test_indices).tolist()\n",
    "    \n",
    "    absence_test_indices.extend(test_indices)\n",
    "    absence_train_indices.extend(train_indices)\n",
    "\n",
    "print(f\"Absence points for testing: {len(absence_test_indices)}\")\n",
    "print(f\"Absence points for training: {len(absence_train_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Presences\n",
    "\n",
    "1. Group by zone and period\n",
    "2. Sample 20% from each group for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group presences by zone and period\n",
    "presence_test_indices = []\n",
    "presence_train_indices = []\n",
    "\n",
    "for zone in presence_gdf['Zone'].unique():\n",
    "    for period in presence_gdf['Period'].unique():\n",
    "        group = presence_gdf[(presence_gdf['Zone'] == zone) & (presence_gdf['Period'] == period)]\n",
    "        \n",
    "        if len(group) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Sample 20% for testing\n",
    "        test_size = max(1, int(0.2 * len(group)))\n",
    "        test_indices = group.sample(n=test_size, random_state=RANDOM_SEED).index.tolist()\n",
    "        train_indices = group.index.difference(test_indices).tolist()\n",
    "        \n",
    "        presence_test_indices.extend(test_indices)\n",
    "        presence_train_indices.extend(train_indices)\n",
    "\n",
    "print(f\"Presence points for testing: {len(presence_test_indices)}\")\n",
    "print(f\"Presence points for training: {len(presence_train_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Save Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine test and train indices\n",
    "test_indices = presence_test_indices + absence_test_indices\n",
    "train_indices = presence_train_indices + absence_train_indices\n",
    "\n",
    "# Create test and train dataframes\n",
    "test_gdf = gdf.loc[test_indices].copy()\n",
    "train_gdf = gdf.loc[train_indices].copy()\n",
    "\n",
    "# Verify the split\n",
    "print(f\"Total points: {len(gdf)}\")\n",
    "print(f\"Training points: {len(train_gdf)} ({len(train_gdf)/len(gdf)*100:.1f}%)\")\n",
    "print(f\"Testing points: {len(test_gdf)} ({len(test_gdf)/len(gdf)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in each split\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(train_gdf['Presence'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(test_gdf['Presence'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the train/test split\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot training points\n",
    "plt.scatter(train_gdf[train_gdf['Presence'] == 1].geometry.x, \n",
    "            train_gdf[train_gdf['Presence'] == 1].geometry.y, \n",
    "            c='blue', marker='^', alpha=0.7, label='Train - Presence')\n",
    "plt.scatter(train_gdf[train_gdf['Presence'] == 0].geometry.x, \n",
    "            train_gdf[train_gdf['Presence'] == 0].geometry.y, \n",
    "            c='lightblue', marker='o', alpha=0.7, label='Train - Absence')\n",
    "\n",
    "# Plot testing points\n",
    "plt.scatter(test_gdf[test_gdf['Presence'] == 1].geometry.x, \n",
    "            test_gdf[test_gdf['Presence'] == 1].geometry.y, \n",
    "            c='red', marker='^', alpha=0.7, label='Test - Presence')\n",
    "plt.scatter(test_gdf[test_gdf['Presence'] == 0].geometry.x, \n",
    "            test_gdf[test_gdf['Presence'] == 0].geometry.y, \n",
    "            c='pink', marker='o', alpha=0.7, label='Test - Absence')\n",
    "\n",
    "plt.title('Train/Test Split Visualization')\n",
    "plt.xlabel('Easting')\n",
    "plt.ylabel('Northing')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = '../data/processed/splits'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the GeoDataFrames\n",
    "train_gdf.to_file(os.path.join(output_dir, 'train_split.geojson'), driver='GeoJSON')\n",
    "test_gdf.to_file(os.path.join(output_dir, 'test_split.geojson'), driver='GeoJSON')\n",
    "\n",
    "# Save the indices as JSON for reference\n",
    "split_indices = {\n",
    "    'train_indices': train_indices,\n",
    "    'test_indices': test_indices,\n",
    "    'metadata': {\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        'train_size': len(train_indices),\n",
    "        'test_size': len(test_indices),\n",
    "        'train_presence_count': len(train_gdf[train_gdf['Presence'] == 1]),\n",
    "        'train_absence_count': len(train_gdf[train_gdf['Presence'] == 0]),\n",
    "        'test_presence_count': len(test_gdf[test_gdf['Presence'] == 1]),\n",
    "        'test_absence_count': len(test_gdf[test_gdf['Presence'] == 0]),\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, 'split_indices.json'), 'w') as f:\n",
    "    json.dump(split_indices, f, indent=2)\n",
    "\n",
    "print(f\"Splits saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hugging Face Dataset Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Markdown documentation for Hugging Face\n",
    "hf_markdown = f\"\"\"\n",
    "# MPG Ranch Horse Detection Dataset - Train/Test Split\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "This dataset contains aerial imagery tiles for horse presence/absence detection at MPG Ranch. \n",
    "The data has been split into training and testing sets according to specific criteria.\n",
    "\n",
    "## Split Methodology\n",
    "\n",
    "The dataset was split into training (80%) and testing (20%) sets using the following criteria:\n",
    "\n",
    "1. **Random Seed**: {RANDOM_SEED} (for reproducibility)\n",
    "2. **Absence Points**: \n",
    "   - Excluded any absence found within 2 meters of a presence (at any time point)\n",
    "   - Grouped by 10 spatial blocks created using K-means clustering\n",
    "   - Sampled 20% randomly from each spatial block for testing\n",
    "3. **Presence Points**:\n",
    "   - Grouped by zone and period\n",
    "   - Sampled 20% randomly from each group for testing\n",
    "\n",
    "## Split Statistics\n",
    "\n",
    "- **Total Points**: {len(gdf)}\n",
    "- **Training Set**: {len(train_gdf)} points ({len(train_gdf)/len(gdf)*100:.1f}%)\n",
    "  - Presence: {len(train_gdf[train_gdf['Presence'] == 1])}\n",
    "  - Absence: {len(train_gdf[train_gdf['Presence'] == 0])}\n",
    "- **Testing Set**: {len(test_gdf)} points ({len(test_gdf)/len(gdf)*100:.1f}%)\n",
    "  - Presence: {len(test_gdf[test_gdf['Presence'] == 1])}\n",
    "  - Absence: {len(test_gdf[test_gdf['Presence'] == 0])}\n",
    "\n",
    "## Files\n",
    "\n",
    "- `train_split.geojson`: GeoJSON file containing the training points\n",
    "- `test_split.geojson`: GeoJSON file containing the testing points\n",
    "- `split_indices.json`: JSON file containing the indices of the training and testing points\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load the splits\n",
    "train_gdf = gpd.read_file('train_split.geojson')\n",
    "test_gdf = gpd.read_file('test_split.geojson')\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Save the markdown file\n",
    "with open(os.path.join(output_dir, 'README.md'), 'w') as f:\n",
    "    f.write(hf_markdown)\n",
    "\n",
    "print(\"Hugging Face documentation created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpg-horses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
} 
