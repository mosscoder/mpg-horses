{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a056292",
   "metadata": {},
   "source": [
    "# Horse Feature DataFrame Generation with Train-Test Split\n",
    "\n",
    "This notebook contains functions to load and analyze ground truth data for horse detection. It processes GeoJSON data and associated orthomosaic tiles to create a feature dataset suitable for machine learning. The notebook also implements a train-test split and exports the data in a format compatible with Hugging Face datasets.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912d7be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import os\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Constants for data directories relative to project root\n",
    "DATA_DIR = \"../data\"\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "SPLITS_DIR = os.path.join(PROCESSED_DIR, \"splits\")\n",
    "HF_DATA_DIR = os.path.join(DATA_DIR, \"hf_dataset\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(SPLITS_DIR, exist_ok=True)\n",
    "os.makedirs(HF_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6851b",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "These functions handle loading and analyzing the ground truth data from GeoJSON files. The data contains point features representing horse presence/absence observations.\n",
    "\n",
    "### `load_ground_truth`\n",
    "Loads ground truth data from a GeoJSON file into a GeoDataFrame. The function includes basic error handling for file existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(\n",
    "    filepath: str = \"../data/vector/groundtruth.geojson\",\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"Load ground truth data from a GeoJSON file.\"\"\"\n",
    "    path = Path(filepath)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    return gpd.read_file(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3258f7",
   "metadata": {},
   "source": [
    "### `get_point_info`\n",
    "Extracts basic information about point features from the GeoDataFrame, including:\n",
    "- Total number of points\n",
    "- Available columns\n",
    "- Spatial bounds of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f7195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_info(gdf: gpd.GeoDataFrame) -> dict:\n",
    "    \"\"\"Get basic information about point features.\"\"\"\n",
    "    points = gdf[gdf.geometry.type == \"Point\"]\n",
    "    return {\n",
    "        \"total_points\": len(points),\n",
    "        \"columns\": list(points.columns),\n",
    "        \"bounds\": points.total_bounds.tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ground truth data\n",
    "gdf = load_ground_truth()\n",
    "\n",
    "# Get and display basic information\n",
    "info = get_point_info(gdf)\n",
    "print(\"Ground Truth Information:\")\n",
    "for key, value in info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Display the first few rows of the GeoDataFrame\n",
    "print(\"\\nFirst few rows of the ground truth data:\")\n",
    "display(gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0ceb6",
   "metadata": {},
   "source": [
    "## Tile Processing Functions\n",
    "\n",
    "This section contains functions for processing and encoding GeoTIFF tiles. These functions handle the conversion of image data into a format suitable for machine learning.\n",
    "\n",
    "### `encode_tile`\n",
    "Reads a GeoTIFF tile and encodes it as compressed bytes. The function:\n",
    "- Loads the raster data using rasterio\n",
    "- Preserves both pixel data and metadata\n",
    "- Compresses the data using numpy's savez_compressed\n",
    "- Returns the encoded data as bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1933ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tile(tile_path: str) -> bytes:\n",
    "    \"\"\"Read a GeoTIFF tile and encode it as bytes.\"\"\"\n",
    "    if not os.path.exists(tile_path):\n",
    "        raise FileNotFoundError(f\"Tile not found: {tile_path}\")\n",
    "\n",
    "    with rasterio.open(tile_path) as src:\n",
    "        data = src.read()\n",
    "        bio = BytesIO()\n",
    "        np.savez_compressed(bio, data=data, **src.meta)\n",
    "        bio.seek(0)\n",
    "        return bio.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b7991",
   "metadata": {},
   "source": [
    "## Feature DataFrame Creation\n",
    "\n",
    "This section handles the creation of a comprehensive DataFrame that combines ground truth data with orthomosaic information. The process involves:\n",
    "- Converting GeoDataFrame to standard DataFrame\n",
    "- Processing orthomosaic dates and directories\n",
    "- Creating feature rows based on temporal relationships\n",
    "\n",
    "### `create_feature_dataframe`\n",
    "Creates a DataFrame by combining ground truth points with orthomosaic information. Key features:\n",
    "- Uses all available orthomosaics for both presence and absence points\n",
    "- Generates tile paths for each point-orthomosaic combination\n",
    "- Adds an `observation_offset` column, which is a signed integer representing the number of days between the orthomosaic date and the ground truth date\n",
    "  - Positive values indicate the orthomosaic was captured after the ground truth observation\n",
    "  - Negative values indicate the orthomosaic was captured before the ground truth observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ac4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dataframe(\n",
    "    gdf: gpd.GeoDataFrame, tiles_dir: str = \"../data/raster/tiles\"\n",
    ") -> gpd.GeoDataFrame:\n",
    "    # Retain the geometry column\n",
    "    df = gdf.copy()\n",
    "\n",
    "    # Get and sort orthomosaics\n",
    "    orthomosaics = sorted(\n",
    "        [\n",
    "            d\n",
    "            for d in os.listdir(tiles_dir)\n",
    "            if os.path.isdir(os.path.join(tiles_dir, d)) and not d.startswith(\".\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert dates\n",
    "    ortho_dates = pd.to_datetime(\n",
    "        [d.split(\"_\")[0] for d in orthomosaics], format=\"%y%m%d\", utc=True\n",
    "    ).tz_localize(None)\n",
    "\n",
    "    # Create rows for each orthomosaic and calculate observation offset\n",
    "    rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        row_date = pd.to_datetime(row[\"Datetime\"]).tz_localize(None)\n",
    "        # Get the geometry for this row\n",
    "        geometry = row.geometry\n",
    "\n",
    "        for ortho, ortho_date in zip(orthomosaics, ortho_dates):\n",
    "            new_row = row.to_dict()\n",
    "            new_row[\"orthomosaic\"] = ortho\n",
    "            new_row[\"tile_path\"] = os.path.join(\n",
    "                tiles_dir,\n",
    "                ortho,\n",
    "                \"presence\" if row[\"Presence\"] == 1 else \"absence\",\n",
    "                f\"{int(row['idx']):04d}.tif\",\n",
    "            )\n",
    "            # Calculate the observation offset in days as a signed integer\n",
    "            # Positive value means orthomosaic was captured after the ground truth date\n",
    "            # Negative value means orthomosaic was captured before the ground truth date\n",
    "            new_row[\"observation_offset\"] = (ortho_date - row_date).days\n",
    "            # Add the geometry to each new row\n",
    "            new_row[\"geometry\"] = geometry\n",
    "            rows.append(new_row)\n",
    "\n",
    "    # Create a GeoDataFrame from the rows\n",
    "    feature_gdf = gpd.GeoDataFrame(rows)\n",
    "    \n",
    "    # Remove the 'tile_path' column\n",
    "    feature_gdf.drop(columns=['tile_path'], inplace=True)\n",
    "    \n",
    "    return feature_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd1e10",
   "metadata": {},
   "source": [
    "## Train-Test Split Functions\n",
    "\n",
    "This section contains functions for creating a stratified train-test split of the data. The split is designed to maintain the distribution of presence/absence points while ensuring spatial and temporal representativeness.\n",
    "\n",
    "### `filter_absences_near_presences`\n",
    "Filters out absence points that are within a specified distance of any presence point. This helps ensure that absence points are truly representative of areas without horses, rather than being near-misses of presence points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a94ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_absences_near_presences(gdf, buffer_distance=2):\n",
    "    \"\"\"\n",
    "    Filter out absence points that are within buffer_distance meters of any presence point.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame containing both presence and absence points\n",
    "        buffer_distance: Distance in meters to buffer around presence points\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame containing only valid absence points\n",
    "    \"\"\"\n",
    "    # Split into presence and absence\n",
    "    presence_gdf = gdf[gdf['Presence'] == 1]\n",
    "    absence_gdf = gdf[gdf['Presence'] == 0]\n",
    "    \n",
    "    print(f\"Original absence points: {len(absence_gdf)}\")\n",
    "    \n",
    "    # Create buffers around presence points\n",
    "    presence_buffers = presence_gdf.copy()\n",
    "    presence_buffers.geometry = presence_gdf.geometry.buffer(buffer_distance)\n",
    "    \n",
    "    # Find absences that intersect with any presence buffer\n",
    "    valid_absences = absence_gdf.copy()\n",
    "    to_remove = []\n",
    "    \n",
    "    for idx, absence in tqdm(absence_gdf.iterrows(), total=len(absence_gdf), desc=\"Filtering absences\"):\n",
    "        for _, buffer in presence_buffers.iterrows():\n",
    "            if absence.geometry.intersects(buffer.geometry):\n",
    "                to_remove.append(idx)\n",
    "                break\n",
    "    \n",
    "    valid_absences = valid_absences.drop(to_remove)\n",
    "    print(f\"Valid absence points (after removing those within {buffer_distance}m of presences): {len(valid_absences)}\")\n",
    "    \n",
    "    return valid_absences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d239808",
   "metadata": {},
   "source": [
    "### `create_spatial_blocks`\n",
    "Creates spatial blocks using K-means clustering on point coordinates. This function helps ensure that the train-test split is spatially representative by grouping points based on their geographic proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1afe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spatial_blocks(gdf, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Create spatial blocks using K-means clustering on point coordinates.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame containing points\n",
    "        n_clusters: Number of spatial blocks to create\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with an additional 'spatial_block' column\n",
    "    \"\"\"\n",
    "    # Extract coordinates for clustering\n",
    "    coords = np.array([(p.x, p.y) for p in gdf.geometry])\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEED)\n",
    "    clusters = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Add cluster labels to the GeoDataFrame\n",
    "    result_gdf = gdf.copy()\n",
    "    result_gdf['spatial_block'] = clusters\n",
    "    \n",
    "    return result_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344be2d8",
   "metadata": {},
   "source": [
    "### `create_train_test_split`\n",
    "Creates a train-test split according to specific criteria:\n",
    "1. For absences: exclude any absence found within 2 meters of a presence (at any time point), then create spatial blocks using K-means clustering and sample 20% randomly from each block\n",
    "2. For presences: group by zone and period, sampling 20% randomly from each group\n",
    "\n",
    "This approach ensures that the test set is representative of the full dataset across both spatial and temporal dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(gdf, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Create train/test split according to specific criteria:\n",
    "    1. For absences: exclude any absence found within 2 meters of a presence,\n",
    "       then create spatial blocks and sample test_size from each block\n",
    "    2. For presences: group by zone and period, sampling test_size from each group\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame containing both presence and absence points\n",
    "        test_size: Proportion of data to include in the test split\n",
    "        \n",
    "    Returns:\n",
    "        train_gdf, test_gdf: GeoDataFrames for training and testing\n",
    "    \"\"\"\n",
    "    # Split into presence and absence\n",
    "    presence_gdf = gdf[gdf['Presence'] == 1].copy()\n",
    "    absence_gdf = gdf[gdf['Presence'] == 0].copy()\n",
    "    \n",
    "    # Filter absences near presences\n",
    "    valid_absences = filter_absences_near_presences(gdf)\n",
    "    \n",
    "    # Create spatial blocks for absences\n",
    "    absence_blocks = create_spatial_blocks(valid_absences)\n",
    "    \n",
    "    # Initialize lists to store indices\n",
    "    presence_test_indices = []\n",
    "    presence_train_indices = []\n",
    "    absence_test_indices = []\n",
    "    absence_train_indices = []\n",
    "    \n",
    "    # Split presences by zone and period\n",
    "    for (zone, period), group in presence_gdf.groupby(['Zone', 'Period']):\n",
    "        if len(group) > 0:\n",
    "            # Calculate number of test samples for this group\n",
    "            n_test = max(1, int(len(group) * test_size))\n",
    "            \n",
    "            # Randomly select indices for test set\n",
    "            group_indices = group.index.tolist()\n",
    "            test_idx = random.sample(group_indices, n_test)\n",
    "            train_idx = [idx for idx in group_indices if idx not in test_idx]\n",
    "            \n",
    "            # Add to master lists\n",
    "            presence_test_indices.extend(test_idx)\n",
    "            presence_train_indices.extend(train_idx)\n",
    "    \n",
    "    # Split absences by spatial block\n",
    "    for block, group in absence_blocks.groupby('spatial_block'):\n",
    "        if len(group) > 0:\n",
    "            # Calculate number of test samples for this block\n",
    "            n_test = max(1, int(len(group) * test_size))\n",
    "            \n",
    "            # Randomly select indices for test set\n",
    "            group_indices = group.index.tolist()\n",
    "            test_idx = random.sample(group_indices, n_test)\n",
    "            train_idx = [idx for idx in group_indices if idx not in test_idx]\n",
    "            \n",
    "            # Add to master lists\n",
    "            absence_test_indices.extend(test_idx)\n",
    "            absence_train_indices.extend(train_idx)\n",
    "    \n",
    "    # Combine test and train indices\n",
    "    test_indices = presence_test_indices + absence_test_indices\n",
    "    train_indices = presence_train_indices + absence_train_indices\n",
    "    \n",
    "    # Create test and train dataframes\n",
    "    test_gdf = gdf.loc[test_indices].copy()\n",
    "    train_gdf = gdf.loc[train_indices].copy()\n",
    "    \n",
    "    # Print split statistics\n",
    "    print(f\"Total points: {len(gdf)}\")\n",
    "    print(f\"Training points: {len(train_gdf)} ({len(train_gdf)/len(gdf)*100:.1f}%)\")\n",
    "    print(f\"Testing points: {len(test_gdf)} ({len(test_gdf)/len(gdf)*100:.1f}%)\")\n",
    "    \n",
    "    # Check class distribution in each split\n",
    "    print(\"\\nClass distribution in training set:\")\n",
    "    print(train_gdf['Presence'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    print(\"\\nClass distribution in test set:\")\n",
    "    print(test_gdf['Presence'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    return train_gdf, test_gdf, train_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a51a6",
   "metadata": {},
   "source": [
    "### `visualize_train_test_split`\n",
    "Creates a visualization of the train-test split, showing the spatial distribution of presence and absence points in both the training and testing sets. This visualization helps to verify that the spatial stratification is working correctly and that the test set is representative of the overall data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8566cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train_test_split(train_gdf, test_gdf, figsize=(14, 12)):\n",
    "    \"\"\"\n",
    "    Visualize the train-test split on a map.\n",
    "    \n",
    "    Args:\n",
    "        train_gdf: GeoDataFrame containing training points\n",
    "        test_gdf: GeoDataFrame containing testing points\n",
    "        figsize: Figure size as a tuple (width, height)\n",
    "    \"\"\"\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot training points\n",
    "    train_presence = train_gdf[train_gdf['Presence'] == 1]\n",
    "    train_absence = train_gdf[train_gdf['Presence'] == 0]\n",
    "    \n",
    "    # Plot testing points\n",
    "    test_presence = test_gdf[test_gdf['Presence'] == 1]\n",
    "    test_absence = test_gdf[test_gdf['Presence'] == 0]\n",
    "    \n",
    "    # Plot each category with different colors and markers\n",
    "    ax.scatter(train_presence.geometry.x, train_presence.geometry.y, \n",
    "               c='forestgreen', marker='o', s=50, alpha=0.7, label='Train - Presence')\n",
    "    ax.scatter(train_absence.geometry.x, train_absence.geometry.y, \n",
    "               c='lightblue', marker='o', s=50, alpha=0.7, label='Train - Absence')\n",
    "    ax.scatter(test_presence.geometry.x, test_presence.geometry.y, \n",
    "               c='darkgreen', marker='X', s=80, alpha=0.9, label='Test - Presence')\n",
    "    ax.scatter(test_absence.geometry.x, test_absence.geometry.y, \n",
    "               c='darkblue', marker='X', s=80, alpha=0.9, label='Test - Absence')\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('Train-Test Split Visualization', fontsize=16)\n",
    "    ax.set_xlabel('Easting', fontsize=14)\n",
    "    ax.set_ylabel('Northing', fontsize=14)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(fontsize=12, loc='best')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566dffd",
   "metadata": {},
   "source": [
    "## Tile Encoding Functions\n",
    "\n",
    "This section contains functions for batch processing and encoding multiple tiles. These functions handle the bulk conversion of image data and provide progress tracking and error handling.\n",
    "\n",
    "### `encode_tile`\n",
    "Reads a GeoTIFF tile and encodes it as compressed bytes. The function:\n",
    "- Loads the raster data using rasterio\n",
    "- Preserves both pixel data and metadata\n",
    "- Compresses the data using numpy's savez_compressed\n",
    "- Returns the encoded data as bytes\n",
    "\n",
    "### `encode_all_tiles`\n",
    "Processes and encodes all tiles in the DataFrame. Key features:\n",
    "- Uses tqdm for progress tracking\n",
    "- Handles encoding errors gracefully\n",
    "- Provides summary statistics of successful/failed encodings\n",
    "- Returns a filtered DataFrame containing only successfully encoded tiles\n",
    "- Includes batch processing capability for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_all_tiles(df: pd.DataFrame, batch_size: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"Encode all tiles in the DataFrame.\"\"\"\n",
    "    result_df = df.copy()\n",
    "    result_df[\"encoded_tile\"] = None\n",
    "    failed_encodings = []\n",
    "\n",
    "    for idx in tqdm(df.index, desc=\"Encoding tiles\"):\n",
    "        try:\n",
    "            result_df.loc[idx, \"encoded_tile\"] = encode_tile(df.loc[idx, \"tile_path\"])\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            failed_encodings.append((df.loc[idx, \"tile_path\"], str(e)))\n",
    "\n",
    "    if failed_encodings:\n",
    "        print(f\"\\nFailed to encode {len(failed_encodings)} tiles\")\n",
    "\n",
    "    success_mask = result_df[\"encoded_tile\"].notna()\n",
    "    print(f\"\\nSuccessfully encoded {success_mask.sum()} out of {len(df)} tiles\")\n",
    "    return result_df[success_mask].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405766d",
   "metadata": {},
   "source": [
    "## Data Storage Functions\n",
    "\n",
    "This section handles the efficient storage of processed data. The functions here manage the serialization of DataFrames into parquet format for use with Hugging Face datasets.\n",
    "\n",
    "### `save_to_parquet`\n",
    "Saves a GeoDataFrame to a single parquet file. Key features:\n",
    "- Creates directory structure if it doesn't exist\n",
    "- Converts geometry to WKT format for storage\n",
    "- Saves the DataFrame as a parquet file\n",
    "- Reports the number of rows saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(gdf, output_path, convert_geometry=True):\n",
    "    \"\"\"\n",
    "    Save GeoDataFrame to a single parquet file.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame to save\n",
    "        output_path: Path to save the parquet file\n",
    "        convert_geometry: Whether to convert geometry to WKT\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Convert geometry to WKT for storage if needed\n",
    "    df = gdf.copy()\n",
    "    if convert_geometry and 'geometry' in df.columns:\n",
    "        df['geometry'] = df['geometry'].apply(lambda geom: geom.wkt if geom else None)\n",
    "    \n",
    "    # Save to parquet\n",
    "    df.to_parquet(output_path, index=True)\n",
    "    print(f\"Saved {len(df)} rows to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7155d677",
   "metadata": {},
   "source": [
    "### `create_hf_documentation`\n",
    "Creates Markdown documentation for the Hugging Face dataset. The documentation includes:\n",
    "- Dataset description\n",
    "- Split methodology\n",
    "- Statistics about the train and test sets\n",
    "- Information about the data files\n",
    "- Usage examples\n",
    "\n",
    "This documentation helps users understand the dataset structure and how to use it with the Hugging Face datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_documentation(train_gdf, test_gdf, train_indices, test_indices, output_path):\n",
    "    \"\"\"\n",
    "    Create Markdown documentation for Hugging Face dataset.\n",
    "    \n",
    "    Args:\n",
    "        train_gdf: Training GeoDataFrame\n",
    "        test_gdf: Testing GeoDataFrame\n",
    "        train_indices: List of training indices\n",
    "        test_indices: List of testing indices\n",
    "        output_path: Path to save the markdown file\n",
    "    \"\"\"\n",
    "    # Create Markdown documentation\n",
    "    hf_markdown = f\"\"\"\n",
    "# MPG Ranch Horse Detection Dataset\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "This dataset contains aerial imagery tiles for horse presence/absence detection at MPG Ranch. \n",
    "The data has been split into training and testing sets according to specific criteria.\n",
    "\n",
    "## Split Methodology\n",
    "\n",
    "The dataset was split into training (80%) and testing (20%) sets using the following criteria:\n",
    "\n",
    "1. **Random Seed**: {RANDOM_SEED} (for reproducibility)\n",
    "2. **Absence Points**: \n",
    "   - Excluded any absence found within 2 meters of a presence (at any time point)\n",
    "   - Grouped by spatial blocks created using K-means clustering\n",
    "   - Sampled 20% randomly from each spatial block for testing\n",
    "3. **Presence Points**:\n",
    "   - Grouped by zone and period\n",
    "   - Sampled 20% randomly from each group for testing\n",
    "\n",
    "## Split Statistics\n",
    "\n",
    "- **Total Points**: {len(train_gdf) + len(test_gdf)}\n",
    "- **Training Set**: {len(train_gdf)} points ({len(train_gdf)/(len(train_gdf) + len(test_gdf))*100:.1f}%)\n",
    "  - Presence: {len(train_gdf[train_gdf['Presence'] == 1])}\n",
    "  - Absence: {len(train_gdf[train_gdf['Presence'] == 0])}\n",
    "- **Testing Set**: {len(test_gdf)} points ({len(test_gdf)/(len(train_gdf) + len(test_gdf))*100:.1f}%)\n",
    "  - Presence: {len(test_gdf[test_gdf['Presence'] == 1])}\n",
    "  - Absence: {len(test_gdf[test_gdf['Presence'] == 0])}\n",
    "\n",
    "## Data Files\n",
    "\n",
    "```yaml\n",
    "data_files:\n",
    "- split: train\n",
    "  path: \"data/train.parquet\"\n",
    "- split: test\n",
    "  path: \"data/test.parquet\"\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"path/to/dataset\")\n",
    "\n",
    "# Access the splits\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    # Save the markdown file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(hf_markdown)\n",
    "    \n",
    "    print(f\"Hugging Face documentation created at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f9294",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline\n",
    "\n",
    "This section executes the complete data processing pipeline:\n",
    "1. Loads and analyzes ground truth data\n",
    "2. Creates the feature DataFrame\n",
    "3. Creates a train-test split\n",
    "4. Saves split information and creates documentation\n",
    "5. Encodes tiles for both train and test sets\n",
    "6. Saves the processed data in parquet format for Hugging Face\n",
    "\n",
    "The pipeline includes comprehensive error handling and progress reporting at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load and analyze ground truth data\n",
    "        gdf = load_ground_truth()\n",
    "\n",
    "        # Print basic information\n",
    "        info = get_point_info(gdf)\n",
    "        print(\"\\nGround Truth Information:\")\n",
    "        for key, value in info.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        # First, filter absences near presences in the original ground truth data\n",
    "        presence_gdf = gdf[gdf['Presence'] == 1].copy()\n",
    "        valid_absences = filter_absences_near_presences(gdf)\n",
    "        \n",
    "        # Create spatial blocks for absences\n",
    "        absence_blocks = create_spatial_blocks(valid_absences)\n",
    "        \n",
    "        # Initialize lists to store indices\n",
    "        presence_test_indices = []\n",
    "        presence_train_indices = []\n",
    "        absence_test_indices = []\n",
    "        absence_train_indices = []\n",
    "        \n",
    "        # Split presences by zone and period\n",
    "        test_size = 0.2\n",
    "        for (zone, period), group in presence_gdf.groupby(['Zone', 'Period']):\n",
    "            if len(group) > 0:\n",
    "                # Calculate number of test samples for this group\n",
    "                n_test = max(1, int(len(group) * test_size))\n",
    "                \n",
    "                # Randomly select indices for test set\n",
    "                group_indices = group.index.tolist()\n",
    "                test_idx = random.sample(group_indices, n_test)\n",
    "                train_idx = [idx for idx in group_indices if idx not in test_idx]\n",
    "                \n",
    "                # Add to master lists\n",
    "                presence_test_indices.extend(test_idx)\n",
    "                presence_train_indices.extend(train_idx)\n",
    "        \n",
    "        # Split absences by spatial block\n",
    "        for block, group in absence_blocks.groupby('spatial_block'):\n",
    "            if len(group) > 0:\n",
    "                # Calculate number of test samples for this block\n",
    "                n_test = max(1, int(len(group) * test_size))\n",
    "                \n",
    "                # Randomly select indices for test set\n",
    "                group_indices = group.index.tolist()\n",
    "                test_idx = random.sample(group_indices, n_test)\n",
    "                train_idx = [idx for idx in group_indices if idx not in test_idx]\n",
    "                \n",
    "                # Add to master lists\n",
    "                absence_test_indices.extend(test_idx)\n",
    "                absence_train_indices.extend(train_idx)\n",
    "        \n",
    "        # Combine test and train indices\n",
    "        gt_test_indices = presence_test_indices + absence_test_indices\n",
    "        gt_train_indices = presence_train_indices + absence_train_indices\n",
    "        \n",
    "        # Create train and test GeoDataFrames from the original ground truth data\n",
    "        gt_test_gdf = gdf.loc[gt_test_indices].copy()\n",
    "        gt_train_gdf = gdf.loc[gt_train_indices].copy()\n",
    "        \n",
    "        # Print split statistics for ground truth data\n",
    "        print(f\"Total ground truth points: {len(gdf)}\")\n",
    "        print(f\"Training points: {len(gt_train_gdf)} ({len(gt_train_gdf)/len(gdf)*100:.1f}%)\")\n",
    "        print(f\"Testing points: {len(gt_test_gdf)} ({len(gt_test_gdf)/len(gdf)*100:.1f}%)\")\n",
    "        \n",
    "        # Save the ground truth GeoDataFrames as GeoJSON for reference\n",
    "        gt_train_gdf.to_file(os.path.join(SPLITS_DIR, 'train_split.geojson'), driver='GeoJSON')\n",
    "        gt_test_gdf.to_file(os.path.join(SPLITS_DIR, 'test_split.geojson'), driver='GeoJSON')\n",
    "        \n",
    "        # Visualize the train-test split on the ground truth data\n",
    "        print(\"\\nVisualizing train-test split...\")\n",
    "        fig, ax = visualize_train_test_split(gt_train_gdf, gt_test_gdf)\n",
    "        plt.show()\n",
    "        \n",
    "        # Now create feature DataFrames for train and test sets separately\n",
    "        print(\"\\nCreating feature DataFrames...\")\n",
    "        train_feature_gdf = create_feature_dataframe(gt_train_gdf)\n",
    "        test_feature_gdf = create_feature_dataframe(gt_test_gdf)\n",
    "        \n",
    "        # Print feature DataFrame info\n",
    "        print(\"\\nTrain Feature DataFrame Info:\")\n",
    "        print(train_feature_gdf.info())\n",
    "        print(\"\\nTest Feature DataFrame Info:\")\n",
    "        print(test_feature_gdf.info())\n",
    "        \n",
    "        # Save the indices as JSON for reference\n",
    "        split_indices = {\n",
    "            'train_indices': gt_train_indices,\n",
    "            'test_indices': gt_test_indices,\n",
    "            'metadata': {\n",
    "                'random_seed': RANDOM_SEED,\n",
    "                'train_size': len(gt_train_indices),\n",
    "                'test_size': len(gt_test_indices),\n",
    "                'train_presence_count': len(gt_train_gdf[gt_train_gdf['Presence'] == 1]),\n",
    "                'train_absence_count': len(gt_train_gdf[gt_train_gdf['Presence'] == 0]),\n",
    "                'test_presence_count': len(gt_test_gdf[gt_test_gdf['Presence'] == 1]),\n",
    "                'test_absence_count': len(gt_test_gdf[gt_test_gdf['Presence'] == 0]),\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(SPLITS_DIR, 'split_indices.json'), 'w') as f:\n",
    "            json.dump(split_indices, f, indent=2)\n",
    "        \n",
    "        # Create Hugging Face documentation\n",
    "        create_hf_documentation(\n",
    "            gt_train_gdf, \n",
    "            gt_test_gdf, \n",
    "            gt_train_indices, \n",
    "            gt_test_indices, \n",
    "            os.path.join(HF_DATA_DIR, 'README.md')\n",
    "        )\n",
    "\n",
    "        # Process and encode tiles for training set\n",
    "        print(\"\\nEncoding training tiles...\")\n",
    "        train_encoded_df = encode_all_tiles(train_feature_gdf)\n",
    "        \n",
    "        # Process and encode tiles for testing set\n",
    "        print(\"\\nEncoding testing tiles...\")\n",
    "        test_encoded_df = encode_all_tiles(test_feature_gdf)\n",
    "        \n",
    "        # Save to parquet files in the format required by Hugging Face\n",
    "        save_to_parquet(train_encoded_df, os.path.join(HF_DATA_DIR, 'train.parquet'))\n",
    "        save_to_parquet(test_encoded_df, os.path.join(HF_DATA_DIR, 'test.parquet'))\n",
    "        \n",
    "        print(\"\\nProcessing complete. Dataset is ready for Hugging Face.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "mpg-horses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
