{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a056292",
   "metadata": {},
   "source": [
    "# Horse Feature DataFrame Generation with Train-Test Split\n",
    "\n",
    "This notebook contains functions to load and analyze ground truth data for horse detection. It processes GeoJSON data and associated orthomosaic tiles to create a feature dataset suitable for machine learning. The notebook also implements a train-test split and exports the data in a format compatible with Hugging Face datasets.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c912d7be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import os\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Constants for data directories relative to project root\n",
    "DATA_DIR = \"../data\"\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "SPLITS_DIR = os.path.join(PROCESSED_DIR, \"splits\")\n",
    "HF_DATA_DIR = os.path.join(DATA_DIR, \"hf_dataset\")\n",
    "RESULTS_DIR = \"../results\"\n",
    "DOCS_DIR = os.path.join(RESULTS_DIR, \"docs\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(SPLITS_DIR, exist_ok=True)\n",
    "os.makedirs(HF_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6851b",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "These functions handle loading and analyzing the ground truth data from GeoJSON files. The data contains point features representing horse presence/absence observations.\n",
    "\n",
    "### `load_ground_truth`\n",
    "Loads ground truth data from a GeoJSON file into a GeoDataFrame. The function includes basic error handling for file existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2359ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(\n",
    "    filepath: str = \"../data/vector/groundtruth.geojson\",\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"Load ground truth data from a GeoJSON file.\"\"\"\n",
    "    path = Path(filepath)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    return gpd.read_file(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3258f7",
   "metadata": {},
   "source": [
    "### `get_point_info`\n",
    "Extracts basic information about point features from the GeoDataFrame, including:\n",
    "- Total number of points\n",
    "- Available columns\n",
    "- Spatial bounds of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16f7195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_info(gdf: gpd.GeoDataFrame) -> dict:\n",
    "    \"\"\"Get basic information about point features.\"\"\"\n",
    "    points = gdf[gdf.geometry.type == \"Point\"]\n",
    "    return {\n",
    "        \"total_points\": len(points),\n",
    "        \"columns\": list(points.columns),\n",
    "        \"bounds\": points.total_bounds.tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a84ee69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Information:\n",
      "total_points: 1600\n",
      "columns: ['idx', 'Presence', 'Zone', 'Period', 'Recency', 'Datetime', 'Latitude', 'Longitude', 'Easting', 'Northing', 'Ellipsoidal_height', 'geometry']\n",
      "bounds: [728103.7675003844, 5173286.410967057, 728857.9386778023, 5173862.10999949]\n",
      "\n",
      "First few rows of the ground truth data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Zone</th>\n",
       "      <th>Period</th>\n",
       "      <th>Recency</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Easting</th>\n",
       "      <th>Northing</th>\n",
       "      <th>Ellipsoidal_height</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-25 06:25:00.600000+00:00</td>\n",
       "      <td>46.678295</td>\n",
       "      <td>-114.007977</td>\n",
       "      <td>728825.999020</td>\n",
       "      <td>5.173764e+06</td>\n",
       "      <td>1182.161</td>\n",
       "      <td>POINT (728825.999 5173763.794)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-25 06:25:24.200000+00:00</td>\n",
       "      <td>46.678261</td>\n",
       "      <td>-114.007970</td>\n",
       "      <td>728826.721929</td>\n",
       "      <td>5.173760e+06</td>\n",
       "      <td>1181.317</td>\n",
       "      <td>POINT (728826.722 5173760.049)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-25 06:26:14+00:00</td>\n",
       "      <td>46.678299</td>\n",
       "      <td>-114.008067</td>\n",
       "      <td>728819.157326</td>\n",
       "      <td>5.173764e+06</td>\n",
       "      <td>1181.695</td>\n",
       "      <td>POINT (728819.157 5173764.064)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-25 06:26:49.400000+00:00</td>\n",
       "      <td>46.678331</td>\n",
       "      <td>-114.008220</td>\n",
       "      <td>728807.322341</td>\n",
       "      <td>5.173767e+06</td>\n",
       "      <td>1180.652</td>\n",
       "      <td>POINT (728807.322 5173767.093)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-04-25 06:27:05.200000+00:00</td>\n",
       "      <td>46.678280</td>\n",
       "      <td>-114.008122</td>\n",
       "      <td>728814.979743</td>\n",
       "      <td>5.173762e+06</td>\n",
       "      <td>1180.683</td>\n",
       "      <td>POINT (728814.98 5173761.785)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  Presence  Zone  Period  Recency                         Datetime  \\\n",
       "0    0         1     1       1        1 2024-04-25 06:25:00.600000+00:00   \n",
       "1    1         1     1       1        1 2024-04-25 06:25:24.200000+00:00   \n",
       "2    2         1     1       1        1        2024-04-25 06:26:14+00:00   \n",
       "3    3         1     1       1        1 2024-04-25 06:26:49.400000+00:00   \n",
       "4    4         1     1       1        2 2024-04-25 06:27:05.200000+00:00   \n",
       "\n",
       "    Latitude   Longitude        Easting      Northing  Ellipsoidal_height  \\\n",
       "0  46.678295 -114.007977  728825.999020  5.173764e+06            1182.161   \n",
       "1  46.678261 -114.007970  728826.721929  5.173760e+06            1181.317   \n",
       "2  46.678299 -114.008067  728819.157326  5.173764e+06            1181.695   \n",
       "3  46.678331 -114.008220  728807.322341  5.173767e+06            1180.652   \n",
       "4  46.678280 -114.008122  728814.979743  5.173762e+06            1180.683   \n",
       "\n",
       "                         geometry  \n",
       "0  POINT (728825.999 5173763.794)  \n",
       "1  POINT (728826.722 5173760.049)  \n",
       "2  POINT (728819.157 5173764.064)  \n",
       "3  POINT (728807.322 5173767.093)  \n",
       "4   POINT (728814.98 5173761.785)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the ground truth data\n",
    "gdf = load_ground_truth()\n",
    "\n",
    "# Get and display basic information\n",
    "info = get_point_info(gdf)\n",
    "print(\"Ground Truth Information:\")\n",
    "for key, value in info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Display the first few rows of the GeoDataFrame\n",
    "print(\"\\nFirst few rows of the ground truth data:\")\n",
    "display(gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0ceb6",
   "metadata": {},
   "source": [
    "## Tile Processing Functions\n",
    "\n",
    "This section contains functions for processing and encoding GeoTIFF tiles. These functions handle the conversion of image data into a format suitable for machine learning.\n",
    "\n",
    "### `encode_tile`\n",
    "Reads a GeoTIFF tile and encodes it as compressed bytes. The function:\n",
    "- Loads the raster data using rasterio\n",
    "- Preserves both pixel data and metadata\n",
    "- Compresses the data using numpy's savez_compressed\n",
    "- Returns the encoded data as bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1933ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tile(tile_path: str) -> bytes:\n",
    "    \"\"\"Read a GeoTIFF tile and encode it as bytes.\"\"\"\n",
    "    if not os.path.exists(tile_path):\n",
    "        raise FileNotFoundError(f\"Tile not found: {tile_path}\")\n",
    "\n",
    "    with rasterio.open(tile_path) as src:\n",
    "        data = src.read()\n",
    "        bio = BytesIO()\n",
    "        np.savez_compressed(bio, data=data, **src.meta)\n",
    "        bio.seek(0)\n",
    "        return bio.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b7991",
   "metadata": {},
   "source": [
    "## Feature DataFrame Creation\n",
    "\n",
    "This section handles the creation of a comprehensive DataFrame that combines ground truth data with orthomosaic information. The process involves:\n",
    "- Converting GeoDataFrame to standard DataFrame\n",
    "- Processing orthomosaic dates and directories\n",
    "- Creating feature rows based on temporal relationships\n",
    "\n",
    "### `create_feature_dataframe`\n",
    "Creates a DataFrame by combining ground truth points with orthomosaic information. Key features:\n",
    "- Uses all available orthomosaics for both presence and absence points\n",
    "- Generates tile paths for each point-orthomosaic combination\n",
    "- Adds an `observation_offset` column, which is a signed integer representing the number of days between the orthomosaic date and the ground truth date\n",
    "  - Positive values indicate the orthomosaic was captured after the ground truth observation\n",
    "  - Negative values indicate the orthomosaic was captured before the ground truth observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60ac4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dataframe(\n",
    "    gdf: gpd.GeoDataFrame, tiles_dir: str = \"../data/raster/tiles\"\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Create a feature DataFrame by combining ground truth points with orthomosaic information.\n",
    "    Only includes combinations where the corresponding tile file exists.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame containing ground truth points\n",
    "        tiles_dir: Directory containing orthomosaic tiles\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with features for each valid point-orthomosaic combination\n",
    "    \"\"\"\n",
    "    # Retain the geometry column\n",
    "    df = gdf.copy()\n",
    "\n",
    "    # Get and sort orthomosaics\n",
    "    orthomosaics = sorted(\n",
    "        [\n",
    "            d\n",
    "            for d in os.listdir(tiles_dir)\n",
    "            if os.path.isdir(os.path.join(tiles_dir, d)) and not d.startswith(\".\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert dates\n",
    "    ortho_dates = pd.to_datetime(\n",
    "        [d.split(\"_\")[0] for d in orthomosaics], format=\"%y%m%d\", utc=True\n",
    "    ).tz_localize(None)\n",
    "\n",
    "    # Create rows for each orthomosaic and calculate observation offset\n",
    "    rows = []\n",
    "    skipped_tiles = 0\n",
    "    total_combinations = 0\n",
    "    \n",
    "    print(\"Creating feature DataFrame with tile validation...\")\n",
    "    \n",
    "    # Determine the correct file naming pattern\n",
    "    # Try to find existing files to determine the pattern\n",
    "    sample_dirs = [os.path.join(tiles_dir, ortho, \"presence\") for ortho in orthomosaics[:2]]\n",
    "    sample_dirs.extend([os.path.join(tiles_dir, ortho, \"absence\") for ortho in orthomosaics[:2]])\n",
    "    \n",
    "    # Look for existing files to determine pattern\n",
    "    file_pattern = None\n",
    "    for sample_dir in sample_dirs:\n",
    "        if os.path.exists(sample_dir):\n",
    "            files = os.listdir(sample_dir)\n",
    "            if files:\n",
    "                # Check if files match the 4-digit pattern\n",
    "                if any(re.match(r'\\d{4}\\.tif', f) for f in files):\n",
    "                    file_pattern = \"{:04d}.tif\"\n",
    "                    print(f\"Using 4-digit file pattern: {file_pattern}\")\n",
    "                    break\n",
    "                # Check if files match a different pattern (e.g., without leading zeros)\n",
    "                elif any(re.match(r'\\d+\\.tif', f) for f in files):\n",
    "                    file_pattern = \"{}.tif\"\n",
    "                    print(f\"Using simple file pattern: {file_pattern}\")\n",
    "                    break\n",
    "    \n",
    "    # Default to 4-digit pattern if we couldn't determine\n",
    "    if not file_pattern:\n",
    "        file_pattern = \"{:04d}.tif\"\n",
    "        print(f\"Could not determine file pattern, defaulting to: {file_pattern}\")\n",
    "    \n",
    "    # Process each point\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing points\"):\n",
    "        row_date = pd.to_datetime(row[\"Datetime\"]).tz_localize(None)\n",
    "        geometry = row.geometry\n",
    "        point_idx = int(row['idx'])\n",
    "        presence_folder = \"presence\" if row[\"Presence\"] == 1 else \"absence\"\n",
    "\n",
    "        for ortho, ortho_date in zip(orthomosaics, ortho_dates):\n",
    "            total_combinations += 1\n",
    "            \n",
    "            # Generate the tile path using the determined pattern\n",
    "            tile_filename = file_pattern.format(point_idx)\n",
    "            tile_path = os.path.join(tiles_dir, ortho, presence_folder, tile_filename)\n",
    "            \n",
    "            # Check if the tile exists before adding it\n",
    "            if os.path.exists(tile_path):\n",
    "                new_row = row.to_dict()\n",
    "                new_row[\"orthomosaic\"] = ortho\n",
    "                new_row[\"tile_path\"] = tile_path\n",
    "                new_row[\"observation_offset\"] = (ortho_date - row_date).days\n",
    "                new_row[\"geometry\"] = geometry\n",
    "                rows.append(new_row)\n",
    "            else:\n",
    "                skipped_tiles += 1\n",
    "\n",
    "    # Create a GeoDataFrame from the rows\n",
    "    feature_gdf = gpd.GeoDataFrame(rows)\n",
    "    \n",
    "    print(f\"Created feature DataFrame with {len(feature_gdf)} valid point-orthomosaic combinations\")\n",
    "    print(f\"Skipped {skipped_tiles} combinations (out of {total_combinations}) due to missing tiles\")\n",
    "    \n",
    "    return feature_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd1e10",
   "metadata": {},
   "source": [
    "## Train-Test Split Functions\n",
    "\n",
    "This section contains functions for creating a stratified train-test split of the data. The split is designed to maintain the distribution of presence/absence points while ensuring spatial and temporal representativeness.\n",
    "\n",
    "### `filter_absences_near_presences`\n",
    "Filters out absence points that are within a specified distance of any presence point. This helps ensure that absence points are truly representative of areas without horses, rather than being near-misses of presence points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a94ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_absences_near_presences(gdf, buffer_distance=2):\n",
    "    \"\"\"\n",
    "    Filter out absence points that are within buffer_distance meters of any presence point.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame containing both presence and absence points\n",
    "        buffer_distance: Distance in meters to buffer around presence points\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame containing only valid absence points\n",
    "    \"\"\"\n",
    "    # Split into presence and absence\n",
    "    presence_gdf = gdf[gdf['Presence'] == 1]\n",
    "    absence_gdf = gdf[gdf['Presence'] == 0]\n",
    "    \n",
    "    print(f\"Original absence points: {len(absence_gdf)}\")\n",
    "    \n",
    "    # Create buffers around presence points\n",
    "    presence_buffers = presence_gdf.copy()\n",
    "    presence_buffers.geometry = presence_gdf.geometry.buffer(buffer_distance)\n",
    "    \n",
    "    # Find absences that intersect with any presence buffer\n",
    "    valid_absences = absence_gdf.copy()\n",
    "    to_remove = []\n",
    "    \n",
    "    for idx, absence in tqdm(absence_gdf.iterrows(), total=len(absence_gdf), desc=\"Filtering absences\"):\n",
    "        for _, buffer in presence_buffers.iterrows():\n",
    "            if absence.geometry.intersects(buffer.geometry):\n",
    "                to_remove.append(idx)\n",
    "                break\n",
    "    \n",
    "    valid_absences = valid_absences.drop(to_remove)\n",
    "    print(f\"Valid absence points (after removing those within {buffer_distance}m of presences): {len(valid_absences)}\")\n",
    "    \n",
    "    return valid_absences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d239808",
   "metadata": {},
   "source": [
    "### `create_spatial_blocks`\n",
    "Creates spatial blocks using K-means clustering on point coordinates. This function helps ensure that the train-test split is spatially representative by grouping points based on their geographic proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1afe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spatial_blocks(gdf, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Create spatial blocks using K-means clustering on point coordinates.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame containing points\n",
    "        n_clusters: Number of spatial blocks to create\n",
    "        \n",
    "    Returns:\n",
    "        GeoDataFrame with an additional 'spatial_block' column\n",
    "    \"\"\"\n",
    "    # Extract coordinates for clustering\n",
    "    coords = np.array([(p.x, p.y) for p in gdf.geometry])\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEED)\n",
    "    clusters = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Add cluster labels to the GeoDataFrame\n",
    "    result_gdf = gdf.copy()\n",
    "    result_gdf['spatial_block'] = clusters\n",
    "    \n",
    "    return result_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344be2d8",
   "metadata": {},
   "source": [
    "### `create_train_test_split`\n",
    "Creates a train-test split according to specific criteria:\n",
    "1. For absences: exclude any absence found within 2 meters of a presence (at any time point), then create spatial blocks using K-means clustering and sample 20% randomly from each block\n",
    "2. For presences: group by zone and period, sampling 20% randomly from each group\n",
    "\n",
    "This approach ensures that the test set is representative of the full dataset across both spatial and temporal dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db47d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(gdf, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Create train/test split according to specific criteria:\n",
    "    1. For absences: exclude any absence found within 2 meters of a presence,\n",
    "       then create spatial blocks and sample test_size from each block\n",
    "    2. For presences: group by zone and period, sampling test_size from each group\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame containing both presence and absence points\n",
    "        test_size: Proportion of data to include in the test split\n",
    "        \n",
    "    Returns:\n",
    "        train_gdf, test_gdf: GeoDataFrames for training and testing\n",
    "    \"\"\"\n",
    "    # Split into presence and absence\n",
    "    presence_gdf = gdf[gdf['Presence'] == 1].copy()\n",
    "    absence_gdf = gdf[gdf['Presence'] == 0].copy()\n",
    "    \n",
    "    # Filter absences near presences\n",
    "    valid_absences = filter_absences_near_presences(gdf)\n",
    "    \n",
    "    # Create spatial blocks for absences\n",
    "    absence_blocks = create_spatial_blocks(valid_absences)\n",
    "    \n",
    "    # Initialize lists to store indices\n",
    "    presence_test_indices = []\n",
    "    presence_train_indices = []\n",
    "    absence_test_indices = []\n",
    "    absence_train_indices = []\n",
    "    \n",
    "    # Split presences by zone and period\n",
    "    for (zone, period), group in presence_gdf.groupby(['Zone', 'Period']):\n",
    "        if len(group) > 0:\n",
    "            # Calculate number of test samples for this group\n",
    "            n_test = max(1, int(len(group) * test_size))\n",
    "            \n",
    "            # Randomly select indices for test set\n",
    "            group_indices = group.index.tolist()\n",
    "            test_idx = random.sample(group_indices, n_test)\n",
    "            train_idx = [idx for idx in group_indices if idx not in test_idx]\n",
    "            \n",
    "            # Add to master lists\n",
    "            presence_test_indices.extend(test_idx)\n",
    "            presence_train_indices.extend(train_idx)\n",
    "    \n",
    "    # Split absences by spatial block\n",
    "    for block, group in absence_blocks.groupby('spatial_block'):\n",
    "        if len(group) > 0:\n",
    "            # Calculate number of test samples for this block\n",
    "            n_test = max(1, int(len(group) * test_size))\n",
    "            \n",
    "            # Randomly select indices for test set\n",
    "            group_indices = group.index.tolist()\n",
    "            test_idx = random.sample(group_indices, n_test)\n",
    "            train_idx = [idx for idx in group_indices if idx not in test_idx]\n",
    "            \n",
    "            # Add to master lists\n",
    "            absence_test_indices.extend(test_idx)\n",
    "            absence_train_indices.extend(train_idx)\n",
    "    \n",
    "    # Combine test and train indices\n",
    "    test_indices = presence_test_indices + absence_test_indices\n",
    "    train_indices = presence_train_indices + absence_train_indices\n",
    "    \n",
    "    # Create test and train dataframes\n",
    "    test_gdf = gdf.loc[test_indices].copy()\n",
    "    train_gdf = gdf.loc[train_indices].copy()\n",
    "    \n",
    "    # Print split statistics\n",
    "    print(f\"Total points: {len(gdf)}\")\n",
    "    print(f\"Training points: {len(train_gdf)} ({len(train_gdf)/len(gdf)*100:.1f}%)\")\n",
    "    print(f\"Testing points: {len(test_gdf)} ({len(test_gdf)/len(gdf)*100:.1f}%)\")\n",
    "    \n",
    "    # Check class distribution in each split\n",
    "    print(\"\\nClass distribution in training set:\")\n",
    "    print(train_gdf['Presence'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    print(\"\\nClass distribution in test set:\")\n",
    "    print(test_gdf['Presence'].value_counts(normalize=True) * 100)\n",
    "    \n",
    "    return train_gdf, test_gdf, train_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a51a6",
   "metadata": {},
   "source": [
    "### `visualize_train_test_split`\n",
    "Creates a visualization of the train-test split, showing the spatial distribution of presence and absence points in both the training and testing sets. This visualization helps to verify that the spatial stratification is working correctly and that the test set is representative of the overall data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8566cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train_test_split(train_gdf, test_gdf, figsize=(14, 12)):\n",
    "    \"\"\"\n",
    "    Visualize the train-test split on a map.\n",
    "    \n",
    "    Args:\n",
    "        train_gdf: GeoDataFrame containing training points\n",
    "        test_gdf: GeoDataFrame containing testing points\n",
    "        figsize: Figure size as a tuple (width, height)\n",
    "    \"\"\"\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Plot training points\n",
    "    train_presence = train_gdf[train_gdf['Presence'] == 1]\n",
    "    train_absence = train_gdf[train_gdf['Presence'] == 0]\n",
    "    \n",
    "    # Plot testing points\n",
    "    test_presence = test_gdf[test_gdf['Presence'] == 1]\n",
    "    test_absence = test_gdf[test_gdf['Presence'] == 0]\n",
    "    \n",
    "    # Plot each category with different colors and markers\n",
    "    ax.scatter(train_presence.geometry.x, train_presence.geometry.y, \n",
    "               c='forestgreen', marker='o', s=50, alpha=0.7, label='Train - Presence')\n",
    "    ax.scatter(train_absence.geometry.x, train_absence.geometry.y, \n",
    "               c='lightblue', marker='o', s=50, alpha=0.7, label='Train - Absence')\n",
    "    ax.scatter(test_presence.geometry.x, test_presence.geometry.y, \n",
    "               c='darkgreen', marker='X', s=80, alpha=0.9, label='Test - Presence')\n",
    "    ax.scatter(test_absence.geometry.x, test_absence.geometry.y, \n",
    "               c='darkblue', marker='X', s=80, alpha=0.9, label='Test - Absence')\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax.set_title('Train-Test Split Visualization', fontsize=16)\n",
    "    ax.set_xlabel('Easting', fontsize=14)\n",
    "    ax.set_ylabel('Northing', fontsize=14)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(fontsize=12, loc='best')\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566dffd",
   "metadata": {},
   "source": [
    "## Tile Encoding Functions\n",
    "\n",
    "This section contains functions for batch processing and encoding multiple tiles. These functions handle the bulk conversion of image data and provide progress tracking and error handling.\n",
    "\n",
    "### `encode_tile`\n",
    "Reads a GeoTIFF tile and encodes it as compressed bytes. The function:\n",
    "- Loads the raster data using rasterio\n",
    "- Preserves both pixel data and metadata\n",
    "- Compresses the data using numpy's savez_compressed\n",
    "- Returns the encoded data as bytes\n",
    "\n",
    "### `encode_all_tiles`\n",
    "Processes and encodes all tiles in the DataFrame. Key features:\n",
    "- Uses tqdm for progress tracking\n",
    "- Handles encoding errors gracefully\n",
    "- Provides summary statistics of successful/failed encodings\n",
    "- Returns a filtered DataFrame containing only successfully encoded tiles\n",
    "- Includes batch processing capability for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24e772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_all_tiles(df: pd.DataFrame, batch_size: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode all tiles in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing information about tiles\n",
    "        batch_size: Number of tiles to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with encoded tiles\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Check if tile_path exists in the DataFrame\n",
    "    if 'tile_path' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'tile_path' column. Please run create_feature_dataframe first.\")\n",
    "    \n",
    "    # Verify that tile paths exist before attempting to encode\n",
    "    valid_paths = result_df['tile_path'].apply(os.path.exists)\n",
    "    if not valid_paths.all():\n",
    "        invalid_count = (~valid_paths).sum()\n",
    "        print(f\"Warning: {invalid_count} tile paths do not exist. These will be skipped during encoding.\")\n",
    "        # Optionally show some examples\n",
    "        if invalid_count > 0:\n",
    "            invalid_examples = result_df.loc[~valid_paths, 'tile_path'].head(5).tolist()\n",
    "            print(\"Examples of invalid paths:\")\n",
    "            for path in invalid_examples:\n",
    "                print(f\"  - {path}\")\n",
    "    \n",
    "    # Initialize encoded_tile column\n",
    "    result_df[\"encoded_tile\"] = None\n",
    "    failed_encodings = []\n",
    "\n",
    "    # Process tiles\n",
    "    for idx in tqdm(df.index, desc=\"Encoding tiles\"):\n",
    "        try:\n",
    "            tile_path = result_df.loc[idx, \"tile_path\"]\n",
    "            if os.path.exists(tile_path):\n",
    "                result_df.loc[idx, \"encoded_tile\"] = encode_tile(tile_path)\n",
    "            else:\n",
    "                failed_encodings.append((tile_path, \"File not found\"))\n",
    "        except Exception as e:\n",
    "            failed_encodings.append((result_df.loc[idx, \"tile_path\"], str(e)))\n",
    "\n",
    "    if failed_encodings:\n",
    "        print(f\"\\nFailed to encode {len(failed_encodings)} tiles\")\n",
    "        # Print the first few failures\n",
    "        if len(failed_encodings) > 0:\n",
    "            print(\"First few failures:\")\n",
    "            for path, error in failed_encodings[:5]:\n",
    "                print(f\"  - {path}: {error}\")\n",
    "\n",
    "    success_mask = result_df[\"encoded_tile\"].notna()\n",
    "    print(f\"\\nSuccessfully encoded {success_mask.sum()} out of {len(df)} tiles\")\n",
    "    \n",
    "    # Return only successfully encoded tiles\n",
    "    return result_df[success_mask].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405766d",
   "metadata": {},
   "source": [
    "## Data Storage Functions\n",
    "\n",
    "This section handles the efficient storage of processed data. The functions here manage the serialization of DataFrames into parquet format for use with Hugging Face datasets.\n",
    "\n",
    "### `save_to_parquet`\n",
    "Saves a GeoDataFrame to parquet file(s) with automatic versioning for large files. Key features:\n",
    "- Creates directory structure if it doesn't exist\n",
    "- Converts geometry to WKT format for storage\n",
    "- Automatically splits files larger than 500 MB into versioned chunks\n",
    "- Creates metadata JSON file for versioned files with information about chunks\n",
    "- Provides detailed reporting on file sizes and chunk distribution\n",
    "- Supports both size-based and row-based chunking strategies\n",
    "\n",
    "The versioning system helps manage large datasets by:\n",
    "1. Preventing creation of excessively large files\n",
    "2. Maintaining compatibility with systems that have file size limitations\n",
    "3. Enabling more efficient parallel processing of data chunks\n",
    "4. Providing clear metadata for reconstructing the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "032d064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(gdf, output_path, convert_geometry=True, max_size_mb=500, rows_per_file=None):\n",
    "    \"\"\"\n",
    "    Save GeoDataFrame to parquet file(s), with versioning for large files.\n",
    "    \n",
    "    Args:\n",
    "        gdf: GeoDataFrame to save\n",
    "        output_path: Path to save the parquet file (relative to ../data/hf/)\n",
    "        convert_geometry: Whether to convert geometry to WKT\n",
    "        max_size_mb: Maximum file size in MB before splitting into versions\n",
    "        rows_per_file: Optional number of rows per file (overrides max_size_mb)\n",
    "    \"\"\"\n",
    "    # Ensure output path is relative to ../data/hf/\n",
    "    if not output_path.startswith('../data/hf/'):\n",
    "        output_path = os.path.join('../data/hf', output_path)\n",
    "    \n",
    "    # Print DataFrame information for verification\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DataFrame Verification for: {os.path.basename(output_path)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Shape: {gdf.shape} (rows, columns)\")\n",
    "    print(f\"Memory usage: {gdf.memory_usage(deep=True).sum() / (1024 * 1024):.2f} MB\")\n",
    "    print(f\"Column dtypes:\")\n",
    "    for col, dtype in gdf.dtypes.items():\n",
    "        print(f\"  - {col}: {dtype}\")\n",
    "    \n",
    "    # Print sample of data\n",
    "    print(\"\\nData sample (first 5 rows):\")\n",
    "    display(gdf.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = gdf.isna().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\nMissing values by column:\")\n",
    "        for col, count in missing_values.items():\n",
    "            if count > 0:\n",
    "                print(f\"  - {col}: {count} ({count/len(gdf)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"\\nNo missing values found.\")\n",
    "    \n",
    "    # Check if it's a GeoDataFrame with geometry column\n",
    "    if 'geometry' in gdf.columns:\n",
    "        geom_types = gdf['geometry'].apply(lambda x: type(x).__name__ if x else 'None').value_counts()\n",
    "        print(\"\\nGeometry types:\")\n",
    "        for geom_type, count in geom_types.items():\n",
    "            print(f\"  - {geom_type}: {count} ({count/len(gdf)*100:.2f}%)\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Convert geometry to WKT for storage if needed\n",
    "    df = gdf.copy()\n",
    "    if convert_geometry and 'geometry' in df.columns:\n",
    "        df['geometry'] = df['geometry'].apply(lambda geom: geom.wkt if geom else None)\n",
    "    \n",
    "    # Estimate size of DataFrame in memory (rough approximation)\n",
    "    estimated_size_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    print(f\"\\nEstimated DataFrame size: {estimated_size_mb:.2f} MB\")\n",
    "    \n",
    "    # Determine if we need to split the file\n",
    "    if rows_per_file is not None:\n",
    "        # Use specified rows per file\n",
    "        need_versioning = len(df) > rows_per_file\n",
    "    else:\n",
    "        # Use size-based estimation\n",
    "        need_versioning = estimated_size_mb > max_size_mb\n",
    "    \n",
    "    if need_versioning:\n",
    "        # Split into multiple files\n",
    "        if rows_per_file is None:\n",
    "            # Calculate rows per file based on size\n",
    "            rows_per_file = max(1, int(len(df) * (max_size_mb / estimated_size_mb)))\n",
    "        \n",
    "        # Get base filename and extension\n",
    "        base_path, ext = os.path.splitext(output_path)\n",
    "        \n",
    "        # Split and save in chunks\n",
    "        total_chunks = (len(df) + rows_per_file - 1) // rows_per_file  # Ceiling division\n",
    "        \n",
    "        print(f\"Splitting into {total_chunks} files with approximately {rows_per_file} rows each\")\n",
    "        \n",
    "        # Create a metadata file with versioning information\n",
    "        metadata = {\n",
    "            'original_file': os.path.basename(output_path),\n",
    "            'total_rows': len(df),\n",
    "            'total_chunks': total_chunks,\n",
    "            'rows_per_chunk': rows_per_file,\n",
    "            'version_files': []\n",
    "        }\n",
    "        \n",
    "        # Save each chunk\n",
    "        for i in range(total_chunks):\n",
    "            start_idx = i * rows_per_file\n",
    "            end_idx = min((i + 1) * rows_per_file, len(df))\n",
    "            \n",
    "            # Create versioned filename with 2-digit numbering\n",
    "            version_path = f\"{base_path}_{i+1:02d}{ext}\"\n",
    "            \n",
    "            # Save this chunk\n",
    "            chunk_df = df.iloc[start_idx:end_idx]\n",
    "            chunk_df.to_parquet(version_path, index=True)\n",
    "            \n",
    "            # Add to metadata\n",
    "            metadata['version_files'].append({\n",
    "                'filename': os.path.basename(version_path),\n",
    "                'rows': len(chunk_df),\n",
    "                'start_idx': start_idx,\n",
    "                'end_idx': end_idx - 1\n",
    "            })\n",
    "            \n",
    "            print(f\"Saved {len(chunk_df)} rows to {version_path}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = f\"{base_path}_metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved versioning metadata to {metadata_path}\")\n",
    "        print(f\"Total rows saved across {total_chunks} files: {len(df)}\")\n",
    "        \n",
    "    else:\n",
    "        # Save as a single file\n",
    "        df.to_parquet(output_path, index=True)\n",
    "        print(f\"Saved {len(df)} rows to {output_path}\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7155d677",
   "metadata": {},
   "source": [
    "### `create_hf_documentation`\n",
    "Creates Markdown documentation for the Hugging Face dataset. The documentation includes:\n",
    "- Dataset description\n",
    "- Split methodology\n",
    "- Statistics about the train and test sets\n",
    "- Information about the data files\n",
    "- Usage examples\n",
    "\n",
    "This documentation helps users understand the dataset structure and how to use it with the Hugging Face datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1155103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hf_documentation(train_gdf, test_gdf, train_indices, test_indices, output_path):\n",
    "    \"\"\"\n",
    "    Create documentation for the Hugging Face dataset.\n",
    "    \n",
    "    Args:\n",
    "        train_gdf: Training GeoDataFrame\n",
    "        test_gdf: Testing GeoDataFrame\n",
    "        train_indices: Indices used for training\n",
    "        test_indices: Indices used for testing\n",
    "        output_path: Path to save the documentation (relative to ../results/docs/)\n",
    "    \"\"\"\n",
    "    # Ensure output path is relative to ../results/docs/\n",
    "    if not output_path.startswith('../results/docs/'):\n",
    "        output_path = os.path.join(DOCS_DIR, output_path)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Create documentation\n",
    "    doc = f\"\"\"# Horse Detection Dataset\n",
    "\n",
    "This dataset contains ground truth data for horse detection in aerial imagery. The data includes presence/absence points and associated orthomosaic tiles.\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "The dataset is split into training and testing sets:\n",
    "\n",
    "### Training Set\n",
    "- Total points: {len(train_gdf)}\n",
    "- Presence points: {(train_gdf['Presence'] == 1).sum()}\n",
    "- Absence points: {(train_gdf['Presence'] == 0).sum()}\n",
    "\n",
    "### Testing Set\n",
    "- Total points: {len(test_gdf)}\n",
    "- Presence points: {(test_gdf['Presence'] == 1).sum()}\n",
    "- Absence points: {(test_gdf['Presence'] == 0).sum()}\n",
    "\n",
    "## Features\n",
    "\n",
    "The dataset includes the following features:\n",
    "\n",
    "1. `idx`: Unique identifier for each point\n",
    "2. `Presence`: Binary indicator (1 for presence, 0 for absence)\n",
    "3. `Zone`: Zone identifier\n",
    "4. `Period`: Time period identifier\n",
    "5. `Recency`: Recency indicator\n",
    "6. `Datetime`: Timestamp of the observation\n",
    "7. `Latitude`: Latitude coordinate\n",
    "8. `Longitude`: Longitude coordinate\n",
    "9. `Easting`: Easting coordinate (UTM)\n",
    "10. `Northing`: Northing coordinate (UTM)\n",
    "11. `Ellipsoidal_height`: Height above ellipsoid\n",
    "12. `geometry`: WKT representation of the point geometry\n",
    "13. `orthomosaic`: Identifier for the associated orthomosaic\n",
    "14. `observation_offset`: Days between observation and orthomosaic capture\n",
    "15. `encoded_tile`: Base64-encoded image tile\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "The data was collected through aerial surveys conducted at the MPG Ranch. Each point represents a ground truth observation of horse presence or absence.\n",
    "\n",
    "## Usage\n",
    "\n",
    "The dataset can be loaded using the Hugging Face datasets library:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"path/to/dataset\")\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "[Add appropriate license information]\n",
    "\"\"\"\n",
    "    \n",
    "    # Save documentation\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(doc)\n",
    "    \n",
    "    print(f\"Saved documentation to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f9294",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline\n",
    "\n",
    "This section executes the complete data processing pipeline:\n",
    "1. Loads and analyzes ground truth data\n",
    "2. Creates the feature DataFrame\n",
    "3. Creates a train-test split\n",
    "4. Saves split information and creates documentation\n",
    "5. Encodes tiles for both train and test sets\n",
    "6. Saves the processed data in parquet format for Hugging Face\n",
    "\n",
    "The pipeline includes comprehensive error handling and progress reporting at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e4662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ground Truth Information:\n",
      "total_points: 1600\n",
      "columns: ['idx', 'Presence', 'Zone', 'Period', 'Recency', 'Datetime', 'Latitude', 'Longitude', 'Easting', 'Northing', 'Ellipsoidal_height', 'geometry']\n",
      "bounds: [728103.7675003844, 5173286.410967057, 728857.9386778023, 5173862.10999949]\n",
      "Original absence points: 716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering absences:  27%|██▋       | 191/716 [00:03<00:08, 60.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/27/sjjxs9213_d437p6xsq_4jcm0000gp/T/ipykernel_68419/639443980.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nProcessing complete. Dataset is ready for Hugging Face.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mError: \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/27/sjjxs9213_d437p6xsq_4jcm0000gp/T/ipykernel_68419/2423794454.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(gdf, buffer_distance)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mto_remove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsence_gdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsence_gdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Filtering absences\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpresence_buffers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mabsence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mto_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/mpg-horses/lib/python3.13/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[0;32m-> 6284\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6286\u001b[0m         \"\"\"\n\u001b[1;32m   6287\u001b[0m         \u001b[0mAfter\u001b[0m \u001b[0mregular\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0maccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mtry\u001b[0m \u001b[0mlooking\u001b[0m \u001b[0mup\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load and analyze ground truth data\n",
    "        gdf = load_ground_truth()\n",
    "\n",
    "        # Print basic information\n",
    "        info = get_point_info(gdf)\n",
    "        print(\"\\nGround Truth Information:\")\n",
    "        for key, value in info.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        # First, filter absences near presences in the original ground truth data\n",
    "        presence_gdf = gdf[gdf['Presence'] == 1].copy()\n",
    "        valid_absences = filter_absences_near_presences(gdf)\n",
    "        \n",
    "        # Create spatial blocks for absences\n",
    "        absence_blocks = create_spatial_blocks(valid_absences)\n",
    "        \n",
    "        # Initialize lists to store indices\n",
    "        presence_test_indices = []\n",
    "        presence_train_indices = []\n",
    "        absence_test_indices = []\n",
    "        absence_train_indices = []\n",
    "        \n",
    "        # Split presences by zone and period\n",
    "        test_size = 0.2\n",
    "        for (zone, period), group in presence_gdf.groupby(['Zone', 'Period']):\n",
    "            if len(group) > 0:\n",
    "                # Calculate number of test samples for this group\n",
    "                n_test = max(1, int(len(group) * test_size))\n",
    "                \n",
    "                # Randomly select indices for test set\n",
    "                group_indices = group.index.tolist()\n",
    "                test_idx = random.sample(group_indices, n_test)\n",
    "                train_idx = [idx for idx in group_indices if idx not in test_idx]\n",
    "                \n",
    "                # Add to master lists\n",
    "                presence_test_indices.extend(test_idx)\n",
    "                presence_train_indices.extend(train_idx)\n",
    "        \n",
    "        # Split absences by spatial block\n",
    "        for block, group in absence_blocks.groupby('spatial_block'):\n",
    "            if len(group) > 0:\n",
    "                # Calculate number of test samples for this block\n",
    "                n_test = max(1, int(len(group) * test_size))\n",
    "                \n",
    "                # Randomly select indices for test set\n",
    "                group_indices = group.index.tolist()\n",
    "                test_idx = random.sample(group_indices, n_test)\n",
    "                train_idx = [idx for idx in group_indices if idx not in test_idx]\n",
    "                \n",
    "                # Add to master lists\n",
    "                absence_test_indices.extend(test_idx)\n",
    "                absence_train_indices.extend(train_idx)\n",
    "        \n",
    "        # Combine test and train indices\n",
    "        gt_test_indices = presence_test_indices + absence_test_indices\n",
    "        gt_train_indices = presence_train_indices + absence_train_indices\n",
    "        \n",
    "        # Create train and test GeoDataFrames from the original ground truth data\n",
    "        gt_test_gdf = gdf.loc[gt_test_indices].copy()\n",
    "        gt_train_gdf = gdf.loc[gt_train_indices].copy()\n",
    "        \n",
    "        # Print split statistics for ground truth data\n",
    "        print(f\"Total ground truth points: {len(gdf)}\")\n",
    "        print(f\"Training points: {len(gt_train_gdf)} ({len(gt_train_gdf)/len(gdf)*100:.1f}%)\")\n",
    "        print(f\"Testing points: {len(gt_test_gdf)} ({len(gt_test_gdf)/len(gdf)*100:.1f}%)\")\n",
    "        \n",
    "        # Save the ground truth GeoDataFrames as GeoJSON for reference\n",
    "        gt_train_gdf.to_file(os.path.join(SPLITS_DIR, 'train_split.geojson'), driver='GeoJSON')\n",
    "        gt_test_gdf.to_file(os.path.join(SPLITS_DIR, 'test_split.geojson'), driver='GeoJSON')\n",
    "        \n",
    "        # Visualize the train-test split on the ground truth data\n",
    "        print(\"\\nVisualizing train-test split...\")\n",
    "        fig, ax = visualize_train_test_split(gt_train_gdf, gt_test_gdf)\n",
    "        plt.show()\n",
    "        \n",
    "        # Now create feature DataFrames for train and test sets separately\n",
    "        print(\"\\nCreating feature DataFrames...\")\n",
    "        train_feature_gdf = create_feature_dataframe(gt_train_gdf)\n",
    "        test_feature_gdf = create_feature_dataframe(gt_test_gdf)\n",
    "        \n",
    "        # Print feature DataFrame info\n",
    "        print(\"\\nTrain Feature DataFrame Info:\")\n",
    "        print(train_feature_gdf.info())\n",
    "        print(\"\\nTest Feature DataFrame Info:\")\n",
    "        print(test_feature_gdf.info())\n",
    "        \n",
    "        # Save the indices as JSON for reference\n",
    "        split_indices = {\n",
    "            'train_indices': gt_train_indices,\n",
    "            'test_indices': gt_test_indices,\n",
    "            'metadata': {\n",
    "                'random_seed': RANDOM_SEED,\n",
    "                'train_size': len(gt_train_indices),\n",
    "                'test_size': len(gt_test_indices),\n",
    "                'train_presence_count': len(gt_train_gdf[gt_train_gdf['Presence'] == 1]),\n",
    "                'train_absence_count': len(gt_train_gdf[gt_train_gdf['Presence'] == 0]),\n",
    "                'test_presence_count': len(gt_test_gdf[gt_test_gdf['Presence'] == 1]),\n",
    "                'test_absence_count': len(gt_test_gdf[gt_test_gdf['Presence'] == 0]),\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(SPLITS_DIR, 'split_indices.json'), 'w') as f:\n",
    "            json.dump(split_indices, f, indent=2)\n",
    "        \n",
    "        # Create Hugging Face documentation\n",
    "        create_hf_documentation(gt_train_gdf, gt_test_gdf, gt_train_indices, gt_test_indices, 'dataset_documentation.md')\n",
    "\n",
    "        # Process and encode tiles for training set\n",
    "        print(\"\\nEncoding training tiles...\")\n",
    "        train_encoded_df = encode_all_tiles(train_feature_gdf)\n",
    "        \n",
    "        # Process and encode tiles for testing set\n",
    "        print(\"\\nEncoding testing tiles...\")\n",
    "        test_encoded_df = encode_all_tiles(test_feature_gdf)\n",
    "        \n",
    "        # Save to parquet files in the format required by Hugging Face\n",
    "        save_to_parquet(\n",
    "            train_encoded_df, \n",
    "            os.path.join(HF_DATA_DIR, 'train.parquet'),\n",
    "            max_size_mb=500\n",
    "        )\n",
    "        save_to_parquet(\n",
    "            test_encoded_df, \n",
    "            os.path.join(HF_DATA_DIR, 'test.parquet'),\n",
    "            max_size_mb=500\n",
    "        )        \n",
    "        \n",
    "        print(\"\\nProcessing complete. Dataset is ready for Hugging Face.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "mpg-horses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
