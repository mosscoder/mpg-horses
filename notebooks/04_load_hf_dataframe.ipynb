{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a056292",
   "metadata": {},
   "source": [
    "# Horse Feature DataFrame Generation\n",
    "\n",
    "This notebook contains functions to load and analyze ground truth data for horse detection. It processes GeoJSON data and associated orthomosaic tiles to create a feature dataset suitable for machine learning.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912d7be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import os\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38368f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for data directories relative to project root\n",
    "DATA_DIR = \"../data\"\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "ENCODED_TILES_DIR = os.path.join(PROCESSED_DIR, \"encoded_tiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6851b",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "These functions handle loading and analyzing the ground truth data from GeoJSON files. The data contains point features representing horse presence/absence observations.\n",
    "\n",
    "### `load_ground_truth`\n",
    "Loads ground truth data from a GeoJSON file into a GeoDataFrame. The function includes basic error handling for file existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(\n",
    "    filepath: str = \"../data/vector/groundtruth.geojson\",\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"Load ground truth data from a GeoJSON file.\"\"\"\n",
    "    path = Path(filepath)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    return gpd.read_file(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3258f7",
   "metadata": {},
   "source": [
    "### `get_point_info`\n",
    "Extracts basic information about point features from the GeoDataFrame, including:\n",
    "- Total number of points\n",
    "- Available columns\n",
    "- Spatial bounds of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f7195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_info(gdf: gpd.GeoDataFrame) -> dict:\n",
    "    \"\"\"Get basic information about point features.\"\"\"\n",
    "    points = gdf[gdf.geometry.type == \"Point\"]\n",
    "    return {\n",
    "        \"total_points\": len(points),\n",
    "        \"columns\": list(points.columns),\n",
    "        \"bounds\": points.total_bounds.tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ground truth data\n",
    "gdf = load_ground_truth()\n",
    "\n",
    "# Get and display basic information\n",
    "info = get_point_info(gdf)\n",
    "print(\"Ground Truth Information:\")\n",
    "for key, value in info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Display the first few rows of the GeoDataFrame\n",
    "print(\"\\nFirst few rows of the ground truth data:\")\n",
    "display(gdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0ceb6",
   "metadata": {},
   "source": [
    "## Tile Processing Functions\n",
    "\n",
    "This section contains functions for processing and encoding GeoTIFF tiles. These functions handle the conversion of image data into a format suitable for machine learning.\n",
    "\n",
    "### `encode_tile`\n",
    "Reads a GeoTIFF tile and encodes it as compressed bytes. The function:\n",
    "- Loads the raster data using rasterio\n",
    "- Preserves both pixel data and metadata\n",
    "- Compresses the data using numpy's savez_compressed\n",
    "- Returns the encoded data as bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1933ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tile(tile_path: str) -> bytes:\n",
    "    \"\"\"Read a GeoTIFF tile and encode it as bytes.\"\"\"\n",
    "    if not os.path.exists(tile_path):\n",
    "        raise FileNotFoundError(f\"Tile not found: {tile_path}\")\n",
    "\n",
    "    with rasterio.open(tile_path) as src:\n",
    "        data = src.read()\n",
    "        bio = BytesIO()\n",
    "        np.savez_compressed(bio, data=data, **src.meta)\n",
    "        bio.seek(0)\n",
    "        return bio.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b7991",
   "metadata": {},
   "source": [
    "## Feature DataFrame Creation\n",
    "\n",
    "This section handles the creation of a comprehensive DataFrame that combines ground truth data with orthomosaic information. The process involves:\n",
    "- Converting GeoDataFrame to standard DataFrame\n",
    "- Processing orthomosaic dates and directories\n",
    "- Creating feature rows based on temporal relationships\n",
    "\n",
    "### `create_feature_dataframe`\n",
    "Creates a DataFrame by combining ground truth points with orthomosaic information. Key features:\n",
    "- Uses all available orthomosaics for both presence and absence points\n",
    "- Generates tile paths for each point-orthomosaic combination\n",
    "- Adds an `observation_offset` column, which is the unsigned integer number of days between the orthomosaic date and the ground truth date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ac4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dataframe(\n",
    "    gdf: gpd.GeoDataFrame, tiles_dir: str = \"../data/raster/tiles\"\n",
    ") -> gpd.GeoDataFrame:\n",
    "    # Retain the geometry column\n",
    "    df = gdf.copy()\n",
    "\n",
    "    # Get and sort orthomosaics\n",
    "    orthomosaics = sorted(\n",
    "        [\n",
    "            d\n",
    "            for d in os.listdir(tiles_dir)\n",
    "            if os.path.isdir(os.path.join(tiles_dir, d)) and not d.startswith(\".\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Convert dates\n",
    "    ortho_dates = pd.to_datetime(\n",
    "        [d.split(\"_\")[0] for d in orthomosaics], format=\"%y%m%d\", utc=True\n",
    "    ).tz_localize(None)\n",
    "\n",
    "    # Create rows for each orthomosaic and calculate observation offset\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        row_date = pd.to_datetime(row[\"Datetime\"]).tz_localize(None)\n",
    "\n",
    "        for ortho, ortho_date in zip(orthomosaics, ortho_dates):\n",
    "            new_row = row.to_dict()\n",
    "            new_row[\"orthomosaic\"] = ortho\n",
    "            new_row[\"tile_path\"] = os.path.join(\n",
    "                tiles_dir,\n",
    "                ortho,\n",
    "                \"presence\" if row[\"Presence\"] == 1 else \"absence\",\n",
    "                f\"{int(row['idx']):04d}.tif\",\n",
    "            )\n",
    "            # Calculate the observation offset in days\n",
    "            new_row[\"observation_offset\"] = abs((ortho_date - row_date).days)\n",
    "            rows.append(new_row)\n",
    "\n",
    "    # Create a GeoDataFrame from the rows\n",
    "    feature_gdf = gpd.GeoDataFrame(rows, geometry=gdf.geometry)\n",
    "    \n",
    "    # Remove the 'tile_path' column\n",
    "    feature_gdf.drop(columns=['tile_path'], inplace=True)\n",
    "    \n",
    "    return feature_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566dffd",
   "metadata": {},
   "source": [
    "## Tile Encoding Functions\n",
    "\n",
    "This section contains functions for batch processing and encoding multiple tiles. These functions handle the bulk conversion of image data and provide progress tracking and error handling.\n",
    "\n",
    "### `encode_all_tiles`\n",
    "Processes and encodes all tiles in the DataFrame. Key features:\n",
    "- Uses tqdm for progress tracking\n",
    "- Handles encoding errors gracefully\n",
    "- Provides summary statistics of successful/failed encodings\n",
    "- Returns a filtered DataFrame containing only successfully encoded tiles\n",
    "- Includes batch processing capability for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_all_tiles(df: pd.DataFrame, batch_size: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"Encode all tiles in the DataFrame.\"\"\"\n",
    "    result_df = df.copy()\n",
    "    result_df[\"encoded_tile\"] = None\n",
    "    failed_encodings = []\n",
    "\n",
    "    for idx in tqdm(df.index, desc=\"Encoding tiles\"):\n",
    "        try:\n",
    "            result_df.loc[idx, \"encoded_tile\"] = encode_tile(df.loc[idx, \"tile_path\"])\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            failed_encodings.append((df.loc[idx, \"tile_path\"], str(e)))\n",
    "\n",
    "    if failed_encodings:\n",
    "        print(f\"\\nFailed to encode {len(failed_encodings)} tiles\")\n",
    "\n",
    "    success_mask = result_df[\"encoded_tile\"].notna()\n",
    "    print(f\"\\nSuccessfully encoded {success_mask.sum()} out of {len(df)} tiles\")\n",
    "    return result_df[success_mask].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405766d",
   "metadata": {},
   "source": [
    "## Data Storage Functions\n",
    "\n",
    "This section handles the efficient storage of processed data. The functions here manage the serialization of large DataFrames into manageable chunks and maintain metadata about the saved data.\n",
    "\n",
    "### `save_chunked_parquet`\n",
    "Saves the DataFrame to parquet files in chunks for efficient storage and loading. Key features:\n",
    "- Automatically determines optimal chunk size based on target size in MB\n",
    "- Creates directory structure if it doesn't exist\n",
    "- Saves each chunk as a separate parquet file\n",
    "- Generates metadata JSON file containing:\n",
    "  - Number of chunks\n",
    "  - Total number of rows\n",
    "  - Total size in MB\n",
    "  - List of chunk file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunked_parquet(\n",
    "    gdf: gpd.GeoDataFrame, output_dir: str = ENCODED_TILES_DIR, target_size_mb: int = 500\n",
    ") -> None:\n",
    "    \"\"\"Save GeoDataFrame to parquet files in chunks.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert geometry to WKT for storage\n",
    "    df = gdf.copy()\n",
    "    df['geometry'] = df['geometry'].apply(lambda geom: geom.wkt if geom else None)\n",
    "\n",
    "    df_size = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "    n_chunks = int(np.ceil(df_size / target_size_mb))\n",
    "    chunk_size = len(df) // n_chunks\n",
    "\n",
    "    for i in tqdm(range(n_chunks), desc=\"Saving chunks\"):\n",
    "        chunk = df.iloc[i * chunk_size : min((i + 1) * chunk_size, len(df))]\n",
    "        chunk.to_parquet(\n",
    "            os.path.join(output_dir, f\"tiles_chunk_{i:03d}.parquet\"), index=True\n",
    "        )\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"n_chunks\": n_chunks,\n",
    "        \"total_rows\": len(df),\n",
    "        \"total_size_mb\": df_size,\n",
    "        \"chunk_files\": [f\"tiles_chunk_{i:03d}.parquet\" for i in range(n_chunks)],\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(output_dir, \"chunks_metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f9294",
   "metadata": {},
   "source": [
    "## Main Processing Pipeline\n",
    "\n",
    "This section executes the complete data processing pipeline:\n",
    "1. Loads and analyzes ground truth data\n",
    "2. Creates the feature DataFrame\n",
    "3. Displays sample data and statistics\n",
    "4. Encodes all tiles\n",
    "5. Saves the processed data in chunked parquet format\n",
    "\n",
    "The pipeline includes comprehensive error handling and progress reporting at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load and analyze ground truth data\n",
    "        gdf = load_ground_truth()\n",
    "\n",
    "        # Print basic information\n",
    "        info = get_point_info(gdf)\n",
    "        print(\"\\nGround Truth Information:\")\n",
    "        for key, value in info.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "        # Create and display feature DataFrame\n",
    "        df = create_feature_dataframe(gdf)\n",
    "        print(\"\\nFeature DataFrame Info:\")\n",
    "        print(df.info())\n",
    "\n",
    "        # Explicitly print the column names of the Feature DataFrame\n",
    "        print(\"\\nFeature DataFrame Columns:\")\n",
    "        print(df.columns.tolist())\n",
    "\n",
    "        # Show samples of presence and absence rows with observation_offset\n",
    "        print(\"\\nSample of Presence Rows (Presence == 1):\")\n",
    "        presence_sample = df[df[\"Presence\"] == 1].head(3)\n",
    "        print(\n",
    "            presence_sample[\n",
    "                [\"Presence\", \"Datetime\", \"orthomosaic\", \"tile_path\", \"observation_offset\"]\n",
    "            ].to_string()\n",
    "        )\n",
    "\n",
    "        print(\"\\nSample of Absence Rows (Presence == 0):\")\n",
    "        absence_sample = df[df[\"Presence\"] == 0].head(3)\n",
    "        print(\n",
    "            absence_sample[\n",
    "                [\"Presence\", \"Datetime\", \"orthomosaic\", \"tile_path\", \"observation_offset\"]\n",
    "            ].to_string()\n",
    "        )\n",
    "\n",
    "        # Print some statistics\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Total rows: {len(df)}\")\n",
    "        print(f\"Presence rows: {len(df[df['Presence'] == 1])}\")\n",
    "        print(f\"Absence rows: {len(df[df['Presence'] == 0])}\")\n",
    "        print(f\"Unique orthomosaics: {df['orthomosaic'].nunique()}\")\n",
    "\n",
    "        # Encode all tiles\n",
    "        print(\"\\nEncoding all tiles...\")\n",
    "        encoded_df = encode_all_tiles(df)\n",
    "        print(\"\\nEncoded DataFrame Info:\")\n",
    "        print(encoded_df.info(show_counts=True))\n",
    "\n",
    "        # Remove the tile_path column after encoding\n",
    "        encoded_df.drop(columns=[\"tile_path\"], inplace=True)\n",
    "\n",
    "        # Explicitly print the column names of the Feature DataFrame\n",
    "        # to confirm dropped tile_path column\n",
    "        print(\"\\nFeature DataFrame Columns:\")\n",
    "        print(df.columns.tolist())\n",
    "\n",
    "        # Show sample of encoded tile sizes\n",
    "        print(\"\\nSample of encoded tile sizes (bytes):\")\n",
    "        encoded_sizes = encoded_df[\"encoded_tile\"].apply(len)\n",
    "        print(f\"Mean size: {encoded_sizes.mean():.0f}\")\n",
    "        print(f\"Min size: {encoded_sizes.min()}\")\n",
    "        print(f\"Max size: {encoded_sizes.max()}\")\n",
    "        print(f\"Total size: {encoded_sizes.sum() / (1024*1024*1024):.2f} GB\")\n",
    "\n",
    "        # Save encoded tiles to parquet chunks\n",
    "        save_chunked_parquet(encoded_df)  # Using default output directory\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "mpg-horses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
